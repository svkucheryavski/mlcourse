{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Using ANN for image recognition\n",
    "\n",
    "In this class, we will combine the knowledge that we have gained so far, both about images and about ANN classification, and use it for more practical and funny applications. Let's learn how to use ANN to classify images.\n",
    "\n",
    "We will use a [dataset](https://www.kaggle.com/datasets/jorgebuenoperez/datacleaningglassesnoglasses) from [Kaggle](https://www.kaggle.com) consisting of the faces of more than 4000 people. Some of them wearing glasses, some of them not. Let's develop an ANN model that will recognize this.\n",
    "\n",
    "The first step is to gain a bit more knowledge about ANN and learn which type of neural networks is specifically good for images. But before we do this, let's re-create some of the functions we created at the end of the previous class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def table(reference, predicted):\n",
    "    \"\"\" computes contingency table for predicted and reference class label indices \"\"\"\n",
    "    indices = reference.unique()\n",
    "    n = len(indices)\n",
    "    ct = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        ni = sum(reference == indices[i])\n",
    "        for j in range(n):\n",
    "            ct[i, j] = sum((reference == indices[i]) & (predicted == indices[j])) / ni\n",
    "\n",
    "    return ct\n",
    "\n",
    "def ct_heatmap(ct, classes):\n",
    "    \"\"\" shows heatmap for the table \"\"\"\n",
    "    plt.imshow(ct, clim = [0, 1])\n",
    "    plt.colorbar()\n",
    "\n",
    "    n = len(classes)\n",
    "    plt.gca().set_xticks(range(n), classes)\n",
    "    plt.gca().set_yticks(range(n), classes)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            plt.text(i, j, round(ct[j, i], 3), color = \"white\" if ct[j, i] < 0.5 else \"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, we did not copy the methods `train()` and `predict()` here because we are going to learn a new way of training and will write new functions for that.\n",
    "\n",
    "Now let's learn some theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Convolutional Neural Networks (CNNs) are a type of Artificial Neural Networks (ANNs) designed specifically for image classification tasks. CNNs are inspired by the human visual system and are particularly effective for processing grid-like data, such as images (containing pixels).  \n",
    "\n",
    "Why not to use simple ANN's we learned in the previous class? Because they need features as inputs — some measurements which are relevant for classification. In the case of Iris data, we used geometric measurements of the flowers, because they are indeed different for various Iris species.\n",
    "\n",
    "In the case of images, we do not have features, we have images. And what we need is to take the images and compute relevant features that will be used for classification. This is exactly how CNN works.\n",
    "\n",
    "To start, let’s first have a look at the main structure of the CNNs architecture and its main components. A general illustration of a convolutional neural network is shown below:\n",
    "\n",
    "<img src=\"./images/CNN-Architecture.png\" style=\"width:800px; height:350px;\"/>\n",
    "\n",
    "Thereby, CNNs architecture consists of two blocks. The first block is needed to create features from images, and it usually consists of the following components:    \n",
    "\n",
    "* **Input Layer:** the network takes an image as an input, so the input is 2D (for grayscale images) or 3D (for color images), not just a vector of values like in the network we used in the previous class. \n",
    "\n",
    "* **Convolutional Layers:** these layers are the core building blocks of CNNs. They apply different filters (like the ones we learned in the first class) to the input images in order to reveal various patterns and features, such as edges, contours, bright and dark spots, textures, or more complex structures.\n",
    "\n",
    "* **Activation Function:** like in simple ANN, activation functions can also be applied to the output of convolutional layers. They introduce non-linearity, allowing the network to learn more complex relationships in the data.   \n",
    "\n",
    "* **Pooling Layer:** pooling layers are needed to reduce the size of the features created by convolutional layers. They kind of pool the features, keeping only the most important information. \n",
    "\n",
    "These three types of layers have a special property — they take images as input (not necessarily images, it can any 3-way array that has width, heights, and number of channels or slices) and produce images as output. \n",
    "\n",
    "The next block of layers works just with numeric tabulated data, like we used for Iris classification in the previous class. Which means that the output from the previous block must be reshaped from 3D to a simple vector of numbers in order to proceed. This can be done by a special operation called *flattening* (or *unfolding*). After that, the information is sent to the next layers: \n",
    "\n",
    "* **Fully Connected Layers:** they are completely the same as what we used in the previous class for the Iris classification. Take inputs as a set of numbers and produce output as a number as well. Usually supplemented with activation function.\n",
    "\n",
    "* **Output Layers:** again, same as we used before — they collect outputs from the fully connected layers and narrow them down to one or several final outputs.\n",
    "\n",
    "One can say that convolutional and pooling layers are needed to construct different features that represent various properties of the image (e.g., the presence of vertical, horizontal, or diagonal lines, circles, etc.). While the fully connected layers and the output layer use these features to do the classification, like in the case of Iris data.\n",
    "\n",
    "This is a simple data flow in a typical CNN:\n",
    "\n",
    "$Input→Convolution→Pooling→Flatenning→Fully Connected Layers→Output$\n",
    "\n",
    "Now let's learn more about the new layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Convolution\n",
    "\n",
    "What is a *convolution*? Well, you already used convolution in the first class when you tried different filters for images. The operation of computing the intensity of the final pixels based on the linear combination of their neighbors and the weights of the filter is called a *convolution*.\n",
    "\n",
    "In the case of CNN, convolution is a way to compute different features for image pixels. Let's recall how images can be represented as a matrix with numbers:\n",
    "\n",
    "<img src=\"./images/Original Image-Pixels.png\" style=\"width:800px; height:350px;\"/>\n",
    "\n",
    "In this case, instead of 0 and 255, we use values of 0 (for black) and 1 (for white) just for the sake of simplicity.\n",
    "\n",
    "In order to apply a convolution to this simple image, we need to define a *filter* (also known as a *kernel*) — a small grid of numbers that will slide from one pixel to another. These numbers are weights, which are used to compute a weighted sum of the original pixel intensities. The result of this computation is the new value, which we call a *feature*.\n",
    "\n",
    "By applying this operation to every pixel of the original (input) image, we create a *feature map*. You can think of the feature map as another image based on the original one. Here is an example:\n",
    "\n",
    "<img src=\"./images/Filters-General.png\" style=\"width:800px; height:1200px;\"/>\n",
    "\n",
    "And here is an interactive illustration of this process also for a 3x3 filter :\n",
    "\n",
    "<img src=\"./images/Image-Kernel-Filter.gif\" style=\"width:800px; height:300px;\"/>\n",
    "\n",
    "This is exactly what convolution layer does — creates feature maps.\n",
    "\n",
    "At this step, it is recommended to open the third sheet of the [Excel workbook](../mlcourse.xlsm) and play with the filtering/convolution example to refresh how it works. Try, e.g., applying the filter from the illustration above.\n",
    "\n",
    "What makes this procedure different in the case of CNN is that we do not define the weights of the filters, only their size. The weights are automatically created during the learning process. In other words, CNN automatically generates features that are best suited for classification or any other task. It literally learns from the data, we just define the number of layers and the size of the kernels.\n",
    "\n",
    "Once the convolution is done, the network applies an activation function to the feature maps, as we also did for our simple model for Iris classification. For example, if ReLU is used as an activation function, it will replace all negative values in the feature map with 0, as shown below.\n",
    "\n",
    "<img src=\"./images/Filter-Activation2.png\" style=\"width:800px; height:250px;\"/>\n",
    "\n",
    "#### Stride and padding\n",
    "\n",
    "In addition to the filter size, the convolutional layer has two other important parameters — *stride* and *padding*. \n",
    "\n",
    "**Stride** is a step filter that moves inside the image. If stride is equal to one, it simply moves the filter window from one pixel to another. If the stride is larger, it jumps with a bigger step. \n",
    "\n",
    "Here is an illustration from Wikimedia showing convolution with stride equal to 2, so it processes every second row (2 and 4 in this case) and every second column (2 and 4 as well):\n",
    "\n",
    "<img src=\"images/strides.gif\">\n",
    "\n",
    "**Padding** is needed to process all pixels and avoid size reduction after filtration. As you can see in the example above, using a 3x3 filter cannot start from row 1 and column 1. The boundary pixels are not processed because, in this case, part of the filter will be outside of the image. To avoid this situation, one can add padding around the original images (usually filled with zeros).\n",
    "\n",
    "Here is an illustration from Wikimedia where convolution still works with stride = 2, but this time padding is added, so the filter takes rows 1, 3, and 5 and the same columns:\n",
    "\n",
    "<img src=\"images/padding-strides.gif\">\n",
    "\n",
    "Try to implement padding for the filtration example in the Excel workbook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After the activation function is applied to the convolution results, the network continues with pooling. \n",
    "\n",
    "Pooling maintains important information while discarding less relevant details, creating spatial hierarchies of features. The most common form of pooling is \"max pooling\", which keeps the biggest value inside a pooling window. If we take a pooling window of size 2 by 2 and apply it to the activated feature map we produced together at the previous step, the result will look as follows:\n",
    "\n",
    "<img src=\"./images/Pooling-Main.png\" style=\"width:800px; height:1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected layers\n",
    "\n",
    "After pooling, the feature maps are unfolded, or *flattened*, so they look like vectors of numbers (like in tabulated data) and are transferred to a set of fully connected layers, followed by the output layer. The fully connected layer is similar to what we had in the case of Iris data, it consists of linear neurons and an activation function:\n",
    "\n",
    "<img src=\"./images/Fully connected.png\" style=\"width:800px; height:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of CNN in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's build a simple CNN network for the classification of color images using PyTorch. \n",
    "\n",
    "We will assume that the input image has only three channels (RGB). The first convolutional layer will take a three-channel image and produce four different feature maps, like applying four different filters to the same image. Then we will apply an activation function and pool the features using a 2x2 max pooling layer. \n",
    "\n",
    "Because we will also add padding, the convolutional layer will produce a feature map of the same size as the original images. But after the pooling layer, it will become twice as small. If the original images have a size of 256x256 pixels, after pooling, we will get 4 feature maps with 128x128 pixels each. \n",
    "\n",
    "After that, we add the second convolutional layer, which will apply its filters to the feature maps produced and pooled in the previous steps. It will take the four feature maps from the previous layer and produce eight new feature maps as a result. \n",
    "\n",
    "The feature maps will also be pooled, so after pooling we will get 8 feature maps with 64x64 pixels each (for an image of size 256x256), which gives a vector with 32,768 feature values, which must be flattened and sent to the fully connected layers.\n",
    "\n",
    "Here is the full code of our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, width, height):\n",
    "        super().__init__()\n",
    "\n",
    "        # first convolutional layer\n",
    "        # - takes 3 channel image (RGB) and produces 4 channels (maps) with features\n",
    "        # - it uses kernel of size 3x3 and adds a pad of 1 pixel around to keep the same size\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, stride=1, padding=1)        # convolutional layer 1\n",
    "        # max pooling layer\n",
    "        # - has size of 2x2 so it reduces size of feature map twice\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # second convolutional layer\n",
    "        # - takes 4 channels (feature maps) and produce 8 channels with features\n",
    "        # - it uses kernel of size 3x3 and adds a pad of 1 pixel around to keep the same size\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)         # convolutional layer 2\n",
    "\n",
    "        # set of fully connected layers, number of inputs in this case depends on width\n",
    "        # and height of the original image (both will be reduced by 4)\n",
    "        self.fc1 = nn.Linear(8 * width // 4 * height // 4, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        # the output layer in this case has two outputs — one for each class\n",
    "        # the classification decision will be made by taking the biggest of the\n",
    "        # two output values\n",
    "        self.fc3 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))    # send input to 1st convolutional layer and ReLU\n",
    "        x = self.pool(x)             # pool the feature maps from previous layer\n",
    "        x = F.relu(self.conv2(x))    # send pooled features to 2nd convolutional layer and ReLU\n",
    "        x = self.pool(x)             # pool the feature maps again\n",
    "\n",
    "        x = torch.flatten(x, 1)      # flatten the outputs from the previous layer\n",
    "        x = F.relu(self.fc1(x))      # send flattened features to the 1st linear layer + ReLU\n",
    "        x = F.relu(self.fc2(x))      # send output of the 1st layer to the 2nd linear layer + ReLU\n",
    "        y_hat = self.fc3(x)          # send output of the 2nd layer to the output layer\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the model for images of 256x256 pixels size and look at the summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = ImageClassifier(256, 256)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the first convolutional layer should have 12 kernels 3x3 each (one kernel for each input and each output). Plus four bias values which gives: $3 \\times 4 \\times 3 \\times 3 + 4 = 112$ parameters.\n",
    "\n",
    "For the second we have: $4 \\times 8 \\times 3 \\times 3 + 8 = 296$ parameters.\n",
    "\n",
    "And so on, in total the model has more that 30 millions parameters to train! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's learn how we can use the CNN network, we have just defined, for real dataset. \n",
    "\n",
    "\n",
    "### Load images as dataset\n",
    "\n",
    "First of all, it worth to mention, that PyTorch has a special additional library, `torchvision` which helps to load images as datasets, assign them labels, etc. This library also provides a module `transforms` which can apply various transformations, like we did in the first class: crop, resize, etc.\n",
    "\n",
    "For CNN it is important that:\n",
    "\n",
    "1. All images have the same size (same number of pixels).\n",
    "2. All images have the same color model or grayscale format.\n",
    "3. Pixels have intensity between 0 and 1.\n",
    "4. All images are PyTorch tensors.\n",
    "\n",
    "All these can be achieved by defining a sequence of transformation methods from `torchvision.transforms` module, which Torch will automatically apply to each image.\n",
    "\n",
    "Let's look at the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# define same size for all images\n",
    "img_width = 256\n",
    "img_height = 256\n",
    "\n",
    "# define path to folder with images for each subset\n",
    "image_path = \"dataset\"\n",
    "\n",
    "# transformation sequence\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize([img_width, img_height]), # resize so each image has the same size\n",
    "    transforms.ToTensor()                       # convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "# create a dataset based on the image folder structure and defined transformation\n",
    "dataset = datasets.ImageFolder(root=image_path, transform=transform)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load two modules from `torchvision`. Module `dataset` contains methods which can load images from disk, assign labels, transform them into Torch tensors, and combine them into a dataset. Module `transforms`, as mentioned already, contains methods for the transformation of images.\n",
    "\n",
    "Then we define the width and height of the target images in pixels. All images will be rescaled to this size. After that, we define the location of the images on disk (path to folder).\n",
    "\n",
    "If you open the folder `dataset` you can see that inside this folder there are two others. Folder `glasses` contains images of faces with glasses, and folder `noglasses` contains face images without glasses. Check several images from each folder.\n",
    "\n",
    "The method `ImageFolder` that we use to load the images \"knows\" about this. So it will assign all images from the first subfolder to the class label `\"glasses\"` and all images from the second subfolder to the class label `\"no glasses\"`. \n",
    "\n",
    "Let's investigate the dataset a little more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows list of classes\n",
    "dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric labels for each class\n",
    "dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show number of elements in the dataset\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of first 10 images\n",
    "dataset.samples[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, inside the `dataset` we do not have tensors with image pixels, but just a full path to every image and a numeric label, which is connected to the text class label. The images will be loaded during the training and prediction processes. This approach helps to save your computer memory.\n",
    "\n",
    "Let's show some of the images using the PIL library we learned in the first class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_indices = range(0, 4000, 200)\n",
    "list(img_indices)\n",
    "list(range(len(img_indices)))\n",
    "\n",
    "img_indices[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# take every 200th of the first 4000 images\n",
    "img_indices = range(0, 4000, 200)\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "for i in range(len(img_indices)):\n",
    "    path, class_ind = dataset.samples[img_indices[i]]\n",
    "    img = Image.open(path)\n",
    "\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(np.array(img))\n",
    "    plt.axis('off')\n",
    "    plt.title(dataset.classes[class_ind])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's split the whole dataset to training, validation, and test sets. This time we will do it randomly, using function `random_split` from Torch. \n",
    "\n",
    "The ideal split will be to take 70% for training, 20% for validation, and 10% for testing. But our dataset is huge (4000+ images), and using even 70% of it for training will make this process very long until we run it on a powerful computer with GPU. \n",
    "\n",
    "To save time, we will take only 20% (800+) of the images for training , 10% for validation, and 10% for testing. However, because `random_split` requires all percents to sum up to 100%, we will create the fourth subset, `rest_set` which we will simply not use.\n",
    "\n",
    "Later, when you learn all the content, try to increase the training set and see how it affects the model quality. In general, the more data you have, the more efficient the training process will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nall = len(dataset)\n",
    "ntrain = int(nall * 0.20)\n",
    "ntrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "nall = len(dataset)\n",
    "ntrain = int(nall * 0.20)\n",
    "nval = int(nall * 0.10)\n",
    "ntest = int(nall * 0.10)\n",
    "nrest = nall - ntrain - nval - ntest\n",
    "\n",
    "\n",
    "# we need a seed here as well because of random splits\n",
    "torch.manual_seed(12)\n",
    "[train_set, val_set, test_set, rest_set] = random_split(dataset, (ntrain, nval, ntest, nrest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN model\n",
    "\n",
    "The training process for the CNN model is almost identical to the one we used in the previous class. We will introduce two differences, though.\n",
    "\n",
    "The first important difference is that we will not send all images at once to the training process. It would take a lot of memory and computational power. Instead, we will do it in small portions — *batches*. \n",
    "\n",
    "So we will split all training and validation sets into batches and make a loop, so it takes images from the first batch and trains the model using this batch. Then it takes images from the second batch, trains the model with the second batch, and so on.\n",
    "\n",
    "This way of training is more efficient and is also a bit faster. Speed is important because in this case our model is very large and the images contain a lot of pixels, so the training process will be much slower than in the case of Iris classification.\n",
    "\n",
    "In order to use batches, PyTorch has a special class called `DataLoader`. So let's create loaders for the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data loader class\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# define how many images will be in one batch\n",
    "batch_size = 10\n",
    "\n",
    "# create data loaders with this batch size\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is a special parameter, `shuffle`, which is set to `True` for the training set loader. This means that images will be sorted into batches randomly. This is very important as it helps to avoid situations when, for example, all images in one batch contain faces without glasses, so the model cannot learn anything from such a batch.\n",
    "\n",
    "The batch size is another (together with the learning rate) important setting which can influence the training quality, so it makes sense to vary it a little if the quality of the trained model is not satisfactory. \n",
    "\n",
    "These settings, which cannot be optimized automatically and it is your responsibility to find the good ones, are called *hyperparameters*. So any model has *parameters* (such as weights and biases of the neurons), which are estimated automatically during the learning process, and *hyperparameters* (such as learning rate, batch size, number of epochs, etc.), which must be optimized manually by a data scientist.\n",
    "\n",
    "Here is a code that implements batch learning (we also make it as a function like in the previous class). We use the same optimizer and the same loss function, like in the final example with Irises. The process will take up to 10-15 minutes, so it can be a good idea to start it, make sure it works and take a break:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, nepochs = 100, lr = 0.001):\n",
    "    \"\"\" trains CNN model with provided data \"\"\"\n",
    "\n",
    "    # define a loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # define optimizer which will compute gradients — do the learning.\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # prepare arrays for losses\n",
    "    train_losses = np.zeros(nepochs)\n",
    "    val_losses = np.zeros(nepochs)\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(nepochs):  # Number of training epochs\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_data in train_loader:\n",
    "            inputs, labels = batch_data\n",
    "            optimizer.zero_grad()\n",
    "            labels_predicted = model(inputs)\n",
    "            loss = loss_function(labels_predicted, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + loss.item()\n",
    "        train_losses[epoch] = train_loss / len(train_loader)\n",
    "\n",
    "        # validate\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        for batch_data in val_loader:\n",
    "            inputs, labels = batch_data\n",
    "            labels_predicted = model(inputs)\n",
    "            loss = loss_function(labels_predicted, labels)\n",
    "            val_loss = val_loss + loss.item()\n",
    "        val_losses[epoch] = val_loss / len(val_loader)\n",
    "\n",
    "        # show how big the losses are at this epoch\n",
    "        print(f'Epoch {epoch}, train loss: {train_losses[epoch]:.4f} - validation loss {val_losses[epoch]:.4f}')\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed the random numbers generator to get reproducible results\n",
    "torch.manual_seed(12)\n",
    "\n",
    "# initialize the model\n",
    "model = ImageClassifier(img_width, img_height)\n",
    "\n",
    "# train it for 40 epochs and learning rate of 0.01\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader, nepochs = 40, lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the losses values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show plot with losses\n",
    "plt.plot(train_losses, label = \"train\")\n",
    "plt.plot(val_losses, label = \"val\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have several problems. First of all, you can see the strange behaviour of the validation loss, it jumps up and down. Perhaps we need to use a smaller learning rate, we will find this out later. \n",
    "\n",
    "The second problem is that starting from approximately the 20th epoch, the validation loss is going slowly up, so the final model is not the most optimal one. Let's talk about how to handle this problem a bit later but now let's check the performance of the model on the test set.\n",
    "\n",
    "To do this, we need to write a new function for making predictions. As you remember, this time we used datasets based on image folders. These types of datasets contain paths to the images and corresponding labels, and then use data loaders to load the images from disk, preprocess them, split them into batches, and feed the model with the batches.\n",
    "\n",
    "For making predictions, we do not need batches, but using a data loader is still handy as it automates a lot of things. What we can do is create a loader with a single batch, so all images will be in that batch, and use it to make predictions. \n",
    "\n",
    "Here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset):\n",
    "    \"\"\" get ANN model and tensor with predictors and returns predicted class label indices \"\"\"\n",
    "    model.eval()\n",
    "    data_loader = DataLoader(dataset, batch_size = len(dataset))\n",
    "    for inputs, labels in data_loader:\n",
    "        output = model.forward(inputs)\n",
    "        _, predicted_labels = torch.max(output, 1)\n",
    "\n",
    "    return labels, predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this function returns both labels and predicted labels, so we can easily reuse the other functions we created in the last class to check the model performance — the computation of the cross table and the visualization of this table as a heat map.\n",
    "\n",
    "Let's make predictions first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels, labels = predict(model, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = table(labels, predicted_labels)\n",
    "ct_heatmap(ct, dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No bad at all, right? Of course, the result will vary if you comment on `manual_seed()` line and run it several times, because in this case, the weights will be initialized randomly, and every time you run the training process, you will get different performance. \n",
    "\n",
    "Now you have a new achievement — you created and trained a CNN model that can automatically recognize people with glasses. Similar models can do a more useful job, for example, detecting if a car driver uses a mobile phone while driving (you probably heard that this is illegal) or sorting different objects (for example, garbage, vegetables, or something similar).\n",
    "\n",
    "There are still a couple of things to learn, but now it is time for exercise:\n",
    "\n",
    "### Exercise\n",
    "\n",
    "In the previous class, you learned how to save a state dictionary of a model at any stage to a variable (by taking a deep copy of the dictionary). Modify the function `train_model()` in the code block below so it always results in a model with the lowest validation loss.\n",
    "\n",
    "For example, if you run a model for 100 epochs and the lowest validation loss was obtained at epoch 67, the function will save the state of this model, and at the end of the training loop, it will load this state to the current model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def train_model(model, train_loader, val_loader, nepochs = 100, lr = 0.001):\n",
    "    \"\"\" trains CNN model with provided data \"\"\"\n",
    "\n",
    "    # define a loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # define optimizer which will compute gradients — do the learning.\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # prepare arrays for losses\n",
    "    train_losses = np.zeros(nepochs)\n",
    "    val_losses = np.zeros(nepochs)\n",
    "\n",
    "    # HINT: here you need to initialize two variables\n",
    "    # - one will keep the best validation loss value\n",
    "    # - second one will keep the state dictionary of the model you got this loss for\n",
    "    best_model = None\n",
    "    best_loss = 99999999999.0\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(nepochs):  # Number of training epochs\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_data in train_loader:\n",
    "            inputs, labels = batch_data\n",
    "            optimizer.zero_grad()\n",
    "            labels_predicted = model(inputs)\n",
    "            loss = loss_function(labels_predicted, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + loss.item()\n",
    "        train_losses[epoch] = train_loss / len(train_loader)\n",
    "\n",
    "        # validate\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        for batch_data in val_loader:\n",
    "            inputs, labels = batch_data\n",
    "            labels_predicted = model(inputs)\n",
    "            loss = loss_function(labels_predicted, labels)\n",
    "            val_loss = val_loss + loss.item()\n",
    "        val_losses[epoch] = val_loss / len(val_loader)\n",
    "\n",
    "        # show how big the losses are at this epoch\n",
    "        print(f'Epoch {epoch}, train loss: {train_losses[epoch]:.4f} - validation loss {val_losses[epoch]:.4f}')\n",
    "\n",
    "        # HINT:\n",
    "        # here you need to add a condition which will compare current model with the\n",
    "        # best one you got so far. If the current model is better, you save its state as\n",
    "        # the new best. You also need to save the best loss value — this is the way to\n",
    "        # see if the next model will be even better\n",
    "        if val_losses[epoch] < best_loss:\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "            best_loss = val_losses[epoch]\n",
    "\n",
    "    # HINT:\n",
    "    # here you need to load the state of the best model from the loop\n",
    "    # to your model object\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def train_model(model, train_loader, val_loader, nepochs = 100, lr = 0.001):\n",
    "    \"\"\" trains CNN model with provided data \"\"\"\n",
    "\n",
    "    # define a loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # define optimizer which will compute gradients — do the learning.\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # prepare arrays for losses\n",
    "    train_losses = np.zeros(nepochs)\n",
    "    val_losses = np.zeros(nepochs)\n",
    "\n",
    "    # initialize two variables which will keep the best validation loss and the best model state\n",
    "    best_loss = 999999999999\n",
    "    best_model_state = None\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(nepochs):  # Number of training epochs\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_data in train_loader:\n",
    "            inputs, labels = batch_data\n",
    "            optimizer.zero_grad()\n",
    "            labels_predicted = model(inputs)\n",
    "            loss = loss_function(labels_predicted, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + loss.item()\n",
    "        train_losses[epoch] = train_loss / len(train_loader)\n",
    "\n",
    "        # validate\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        for batch_data in val_loader:\n",
    "            inputs, labels = batch_data\n",
    "            labels_predicted = model(inputs)\n",
    "            loss = loss_function(labels_predicted, labels)\n",
    "            val_loss = val_loss + loss.item()\n",
    "        val_losses[epoch] = val_loss / len(val_loader)\n",
    "\n",
    "        # show how big the losses are at this epoch\n",
    "        print(f'Epoch {epoch}, train loss: {train_losses[epoch]:.4f} - validation loss {val_losses[epoch]:.4f}')\n",
    "\n",
    "        # check if validation loss is better to what is known so far\n",
    "        # if so save the model state\n",
    "        if val_losses[epoch] < best_loss:\n",
    "            best_loss = val_losses[epoch]\n",
    "            best_model_state = deepcopy(model.state_dict())\n",
    "\n",
    "    # load the state from the best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you write your function, test it using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed the random numbers generator to get reproducible results\n",
    "torch.manual_seed(12)\n",
    "\n",
    "# initialize the model\n",
    "model = ImageClassifier(img_width, img_height)\n",
    "\n",
    "# train it for 40 epochs and learning rate of 0.01\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader, nepochs = 40, lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute loss of the final model on validation set\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "val_loss = 0\n",
    "model.eval()\n",
    "for batch_data in val_loader:\n",
    "    inputs, labels = batch_data\n",
    "    labels_predicted = model(inputs)\n",
    "    loss = loss_function(labels_predicted, labels)\n",
    "    val_loss = val_loss + loss.item()\n",
    "val_loss = val_loss / len(val_loader)\n",
    "\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see that the loss we got in this test is smaller than the loss shown for the last epoch when we trained the model, it works. \n",
    "\n",
    "Let's visualize this by showing a plot with losses from the training process and the loss of the final model as a horizontal line. If the line touches the validation loss curve at the global minimum, your function works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show plot with losses\n",
    "plt.plot(train_losses, label = \"train\")\n",
    "plt.plot(val_losses, label = \"val\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# show the recently computed loss as horizontal line\n",
    "plt.plot(plt.xlim(), [val_loss, val_loss], color = \"black\", linestyle = \"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions for new images\n",
    "\n",
    "What if we want to make a prediction for a new image? Just one single image whose real label we do not know, so we can not use the data loader in this case. Let's take the image number 2000 from the original dataset set for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset items contain two elements - path and label, so we take the first one\n",
    "new_image_path = dataset.imgs[2000][0]\n",
    "new_image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(new_image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the person wears glasses, let's see how to make predictions without data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually apply all transformations to the new image\n",
    "img_transformed = transform(img)\n",
    "\n",
    "# reshape image tensor because model does not work with single images\n",
    "# it expects tensor of images. So if we need to feed model the image\n",
    "# we make it a tensor with one image\n",
    "img_transformed = img_transformed.reshape(1, 3, img_width, img_height)\n",
    "\n",
    "# apply the model\n",
    "output = model.forward(img_transformed)\n",
    "\n",
    "# compute label\n",
    "_, labels_predicted = torch.max(output, 1)\n",
    "\n",
    "# show all outcomes\n",
    "(output, labels_predicted, dataset.classes[labels_predicted])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply network to camera image\n",
    "\n",
    "Now let's learn how we can use the model in order to make predictions in real time by taking photos with our frontal cameras. Run the next code in order to get the image of your face or the face of your friend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to camera\n",
    "import cv2\n",
    "camera = cv2.VideoCapture(0)\n",
    "camera.isOpened()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a picture (repeat if necessary to get better result)\n",
    "ret, frame = camera.read()\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close connection to the camera\n",
    "camera.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the coordinate axes in order to define a crop rectangle. Make sure the final face is located at the center of the cropped image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left top right bottom\n",
    "crop_rect = [250, 70, 474, 294]\n",
    "img = Image.fromarray(frame)\n",
    "img_cropped = img.crop(crop_rect)\n",
    "print(img_cropped.size)\n",
    "display(img_cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the model and see prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transformed = transform(img_cropped)\n",
    "img_transformed = img_transformed.reshape(1, 3, 256, 256)\n",
    "output = model.forward(img_transformed)\n",
    "\n",
    "# compute label\n",
    "_, labels_predicted = torch.max(output, 1)\n",
    "\n",
    "# show all outcomes\n",
    "(output, labels_predicted, dataset.classes[labels_predicted])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to perform calculations on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that, in contrast to experiments with Iris data, training and using the model for images require much longer. This process can be sped up if you have an NVIDIA GPU on your computer (even the simplest one, like RTX 1060).\n",
    "\n",
    "In order to do this, you need to send your model and your data to the GPU device using a special method. Below, you will find a code that implements this approach. The code is versatile, which means if you have a GPU it will use it but if not, you can run your code on a CPU without changing anything.\n",
    "\n",
    "First of all, let's learn how to detect compatible GPU devices. In terms of Torch, such a device is called [CUDA](https://en.wikipedia.org/wiki/CUDA) (`cuda`). It is a name of the corresponding computer library which lets use GPU devices for calculations. Here is how to check if you have one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this code will automatically define the available device and keep it in separate variable. If you have CUDA it will select it, if not it will select you CPU instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's re-write the training loop in order to use the detected device for training. Here we reuse the data loaders and model class you have created before in this class, so to make it work, you should run the previous code cells.\n",
    "\n",
    "The code is kept as simple as possible without a validation loop, just to give you an idea. As you can see, there are only two changes, one line where we send the model to the device (`model.to(device)`). And second line, where we do the same with inputs and labels (`inputs.to(device)` and `labels.to(device)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model\n",
    "model = ImageClassifier(img_width, img_height)\n",
    "\n",
    "# define number of epochs and learning rate\n",
    "nepochs = 3\n",
    "lr = 0.01\n",
    "\n",
    "# define a loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# define optimizer which will compute gradients — do the learning.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# NEW: send the model to device you defined earlier\n",
    "model.to(device)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(nepochs):  # Number of training epochs\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_data in train_loader:\n",
    "        inputs, labels = batch_data\n",
    "\n",
    "        # NEW: send batch data to the device as well\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        labels_predicted = model(inputs)\n",
    "        loss = loss_function(labels_predicted, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = train_loss + loss.item()\n",
    "\n",
    "    train_losses = train_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch}, train loss: {train_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you train your model on a GPU and want to make predictions, the new data (test set, validation set, or new image) must also be loaded onto the GPU first. Alternatively, you can unload your model from GPU/CUDA to CPU by running: `model.to(torch.device(\"cpu\"))` after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "\n",
    "As you may have noticed, the training process, even for a relatively simple network like the one we created on a relatively small dataset, takes a long time. Using a GPU/CUDA can solve this problem, but only partially. Good models with high accuracy are much more complicated and have been trained on a much larger dataset. Can we use the benefits of the model trained by someone else?\n",
    "\n",
    "Yes, we can, and this is exactly what *transfer learning* does. The idea is that you take a model that has already been trained on a very broad range of images and image classes. And then you fine-tune this model to make it work on your particular dataset.\n",
    "\n",
    "Because the weights of this model are already set, you do not need to start the learning process from the scratch and this saves a lot of resources.\n",
    "\n",
    "Torch has already several pre-treated models, including famoues ones like [AlexNet](https://en.wikipedia.org/wiki/AlexNet), [VGG](https://www.kaggle.com/code/blurredmachine/vggnet-16-architecture-a-complete-guide) and many others. Models for image classification are located in `torchvision.models`. Let's load one of those ([residual network](https://en.wikipedia.org/wiki/Residual_neural_network) with 18 layers, there is also one with 50):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchinfo import summary\n",
    "\n",
    "model = models.resnet18()\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model has fewer parameters than ours, but the architecture is much more complex, which makes it more efficient and versatile (the output is actually truncated, so you do not see the full structure).\n",
    "\n",
    "In order to use the network, we need to know how to transform (preprocess) the images and what the image size should be. Luckily, we can get the already-prepared stack of transformations, which is connected to the weights of the model. \n",
    "\n",
    "Here is how to get it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = models.ResNet18_Weights.DEFAULT\n",
    "transform = weights.transforms()\n",
    "transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the transformation stack is different from what we used before, we need to recreate the dataset, subsets and data loaders. We just repeat the code with new transformation object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset based on the image folder structure and defined transformation\n",
    "dataset = datasets.ImageFolder(root=image_path, transform=transform)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "nall = len(dataset)\n",
    "ntrain = int(nall * 0.20)\n",
    "nval = int(nall * 0.10)\n",
    "ntest = int(nall * 0.10)\n",
    "nrest = nall - ntrain - nval - ntest\n",
    "\n",
    "\n",
    "torch.manual_seed(12)\n",
    "\n",
    "[train_set, val_set, test_set, rest_set] = random_split(dataset, (ntrain, nval, ntest, nrest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data loader class\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# define how many images will be in one batch\n",
    "batch_size = 10\n",
    "\n",
    "# create data loaders with this batch size\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training we actually need to modify the model. The original model can classify images among 1000 classes, so it has 1000 outputs. You can see this if you check the output layer, which has name `fc` in this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can adjust it by creating a new linear layer with as many inputs as the original one and only two outputs. We can replace part of the ResNet18 model. Here is how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the output layer in the ResNet18 model by a new one with 2 outputs\n",
    "in_features = model.fc. in_features\n",
    "model.fc = nn.Linear(in_features, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train, or rather fine-tune, the model. We will start with just 10 epochs. If you have created the smart `train_model()` method in one of the exercises, it will end up with the most optimal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = train_model(model, train_loader, val_loader, nepochs=10, lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, already after 1-2 epochs the loss gets very small. Let's check the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, predicted_labels = predict(model, test_set)\n",
    "ct = table(labels, predicted_labels)\n",
    "ct_heatmap(ct, dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect result and much faster than training the CNN model from the scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do next?\n",
    "\n",
    "If you want to continue and develop your skills further, here are some useful links.\n",
    "\n",
    "To learn Python in a more systematic way, you can look into the following materials:\n",
    "\n",
    "* [Python for kids](https://www.geeksforgeeks.org/python-for-kids/)\n",
    "* [Google Python class](https://developers.google.com/edu/python)\n",
    "* [Python tutorial at geeksforgeeks.org](https://www.geeksforgeeks.org/python-programming-language-tutorial/)\n",
    "* [Scientific Python lectures](https://lectures.scientific-python.org/)\n",
    "\n",
    "There are tonnes more, of course, and Python is probably the most popular programming language nowadays.\n",
    "\n",
    "As for data science, machine learning, or artificial intelligence, there are hundreds of good courses as well. We recommend a [free online course](https://course.fast.ai) from FastAI, which covers all aspects of modern ML/AI including more sophisticated topics like Stable Diffusion. It is also based on Jupyter notebooks, so you will feel comfortable from the start.\n",
    "\n",
    "Happy learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
