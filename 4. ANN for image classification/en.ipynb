{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Using ANN for image recognition\n",
    "\n",
    "In this class we will combine the knowledge that we gained so far, both about images and about ANN classification, and use it for more practical and funny application. Let's learn how to use ANN to classify images.\n",
    "\n",
    "We will use a [dataset](https://www.kaggle.com/datasets/jorgebuenoperez/datacleaningglassesnoglasses) from [Kaggle](https://www.kaggle.com), consisting of faces of more than 4000 people. Some of them wearing glasses, some of them not. Let's develop an ANN model that will recognize this.\n",
    "\n",
    "The first step is to gain a bit more knowledge about ANN and learn which type of neural networks is specifically good for images. But before we do this, let's re-create some of the functions we have created at the end of the previous class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def table(reference, predicted):\n",
    "    \"\"\" computes contingency table for predicted and reference class label indices \"\"\"\n",
    "    indices = reference.unique()\n",
    "    n = len(indices)\n",
    "    ct = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        ni = sum(reference == indices[i])\n",
    "        for j in range(n):\n",
    "            ct[i, j] = sum((reference == indices[i]) & (predicted == indices[j])) / ni\n",
    "\n",
    "    return ct\n",
    "\n",
    "def ct_heatmap(ct, classes):\n",
    "    \"\"\" shows heatmap for the table \"\"\"\n",
    "    plt.imshow(ct, clim = [0, 1])\n",
    "    plt.colorbar()\n",
    "\n",
    "    n = len(classes)\n",
    "    plt.gca().set_xticks(range(n), classes)\n",
    "    plt.gca().set_yticks(range(n), classes)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            plt.text(i, j, round(ct[j, i], 3), color = \"white\" if ct[j, i] < 0.5 else \"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may noticedб we did not copy the methods `train()` and `predict()` here, because we are going to learn a new way of training and will write new functions for that.\n",
    "\n",
    "Now let's learn some theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Convolutional Neural Networks (CNNs) as a type of Artificial Neural Networks (ANNs) designed specifically for image classification tasks. CNNs are inspired by the human visual system and are particularly effective for processing grid-like data, such as images (containing pixels).  \n",
    "\n",
    "Why not to use simple ANN's we have learned in the previous class? Because they need features as inputs — some measurements which are relevant for classification. In case of Iris data we used geometric measurements of the flowers, because they are indeed different for various Iris spieces.\n",
    "\n",
    "In case of images we do not have features, we have images. And what we need, is to take the images and compute relevant features that will be used for classification. This is exactly how CNN works.\n",
    "\n",
    "To start, let’s first have a look at the main structure of the CNNs architecture and its main components. A general illustration of a convolutional neural network is shown below:\n",
    "\n",
    "<img src=\"./images/CNN-Architecture.png\" style=\"width:800px; height:350px;\"/>\n",
    "\n",
    "Thereby CNNs architecture consists of two blocks. The first block is needed to create features from images and it usually consists of the following components:    \n",
    "\n",
    "* **Input Layer:** the network takes an image as an input, so the input is 2D (for grayscale image) or 3D (for color images) not just a vector of values like in the network we used in the previous class. \n",
    "\n",
    "* **Convolutional Layers:** these layers are the core building blocks of CNNs. They apply different filters (like the ones we learned in the first class) to the input images in order to reveal various patterns and features, such as edges, contours, bright and dark spots, textures, or more complex structures.\n",
    "\n",
    "* **Activation Function:** like in simple ANN, activation functions can be also applied to the output of convolutional layers. They introduce non-linearity, allowing the network to learn more complex relationships in the data.   \n",
    "\n",
    "* **Pooling Layer:** pooling layers are needed to reduce the size of the features created by convolutional layers. They kind of pool the features keeping only the most important information. \n",
    "\n",
    "These three types of layers have a special property — they take images as input (not necessarily images, it can any 3-way array, which has width, heights and number of channels or slices) and produce image as an output. \n",
    "\n",
    "The next block of layers work just with numeric tabulated data, like we used for Iris classification in the previous class. Which means that the output from the previous block must be reshaped from 3D to a simple vector of numbers in order to proceed. This can be done by a special operation, which is called *flattening* (or *unfolding*). After that the information is sent to the next layers: \n",
    "\n",
    "* **Fully Connected Layers:** they are completely the same as we used in the previous class for Iris classification. Take inputs as a set of numbers and produce output as a number as well. Usually supplemented with activation function.\n",
    "\n",
    "* **Output Layers:** again same as we used before — they collect outputs from the fully connected layers and narrow it down to one or several final outputs.\n",
    "\n",
    "One can say, that convolutional and pooling layers are needed to construct different feaures, which represent various properties on the image (e.g. presence of vertical, horizontal or diagonal lines, circles, etc). While the fully connected layers and the output layer use these features to do the classification, like in case of Iris data.\n",
    "\n",
    "This is a simple data flow in a typical CNN:\n",
    "\n",
    "$Input→Convolution→Pooling→Flatenning→Fully Connected Layers→Output$\n",
    "\n",
    "Now let's learn more about the new layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Convolution\n",
    "\n",
    "What is a *convolution*? Well you already used convolution in the first class, when tried different filters for images. The operation of computing intensity of the final pixels based on linear combination of its neighbors and weights of the filter is called a *convolution*.\n",
    "\n",
    "In case of CNN, convolution is a way to compute different features for image pixels. Let's recall how images can be represented as a matrix with numbers:\n",
    "\n",
    "<img src=\"./images/Original Image-Pixels.png\" style=\"width:800px; height:350px;\"/>\n",
    "\n",
    "In this case instead of 0 and 255 we use values of 0 (for black) and 1 for (white) just for the sake of simplicity.\n",
    "\n",
    "In order to apply a convolution to this simple image we need to define a *filter* (also known as a *kernel*) — a small grid of numbers, which will slide from one pixel to another. These numbers are weights, which are used to compute a weighted sum of the original pixel intensities. The result of this computation is the new value, which we call a *feature*.\n",
    "\n",
    "By applying this operation to every pixel of the original (input) image we create a *feature map*. You can think about the feature map as another image, based on the original one. Here is an example:\n",
    "\n",
    "<img src=\"./images/Filters-General.png\" style=\"width:800px; height:1200px;\"/>\n",
    "\n",
    "And here is an interactive illustration of this process also for a 3x3 filter :\n",
    "\n",
    "<img src=\"./images/Image-Kernel-Filter.gif\" style=\"width:800px; height:300px;\"/>\n",
    "\n",
    "This is exactly what convolution layer does — creates feature maps.\n",
    "\n",
    "At this step it is recommended to open the third sheet of the [Excel workbook](../mlcourse.xlsm) and play with the filtering/convolution example to refresh how it works. Try e.g. to apply the filter from the illustration above.\n",
    "\n",
    "What makes this procedure different in case of CNN, is that we do not define the weights of the filters, only their size. The weights are automatically created during the learning process. In other words, CNN automatically generates features, which are best suitable for classification or any other tasks. It literally learns from the data, we just define the number of the layers and the size of the kernels.\n",
    "\n",
    "Once the convolution is done, the network applies an activation function to the feature maps as we also did for our simple model for Iris classification. For example, if ReLU is used as activation function it will replace all negative values in the feature map with 0, as shown below.\n",
    "\n",
    "<img src=\"./images/Filter-Activation2.png\" style=\"width:800px; height:250px;\"/>\n",
    "\n",
    "#### Stride and padding\n",
    "\n",
    "In addition to the filter size, convolutional layer has two other important parameters — *stride* and *padding*. \n",
    "\n",
    "**Stride** is a step filter moves inside the image with. If stride is equal to one, it simply moves the filter window from one pixel to another. If stride is larger, it jumps with a bigger step. \n",
    "\n",
    "Here is an illustration from Wikimedia showing convolution with stride equal to 2, so it processes every second row (2 and 4 in this case) and every second column (2 and 4 as well):\n",
    "\n",
    "<img src=\"images/strides.gif\">\n",
    "\n",
    "**Padding** is needed to process all pixels and avoid size reduction after filtration. As you can see in the example above, using 3x3 filter can not start from row 1 and column 1. The boundary pixels are not processed because in this case part of the filter will be outside of the image. To avoid this situation one can add a padding around the original images (usually filled with zeros).\n",
    "\n",
    "Here is an illustration from Wikimedia where convolution still works with stride = 2, but this time padding is added, so the the filter takes rows 1, 3 and 5 and the same columns:\n",
    "\n",
    "<img src=\"images/padding-strides.gif\">\n",
    "\n",
    "Try to implement padding for the filtration example in Excel workbook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After activation function is applied to the convolution results, the network continuous with pooling. \n",
    "\n",
    "Pooling maintains important information while discarding less relevant details, creating spatial hierarchies of features. The most common form of pooling is \"max pooling\", which keeps the biggest value inside a pooling window. If we take a pooling window of size 2 by 2 and apply it to the activated feature map we produced together at the previous step, the result will look as follows:\n",
    "\n",
    "<img src=\"./images/Pooling-Main.png\" style=\"width:800px; height:1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected layers\n",
    "\n",
    "After pooling, the feature maps are unfolded or *flattened*, so they look as vector of numbers (like in tabulated data) and are transferred to a set of fully connected layers, followed by the output layer. The fully connected layer is similar to what we had in case of Iris data, it consists of linear neurons and activation function:\n",
    "\n",
    "<img src=\"./images/Fully connected.png\" style=\"width:800px; height:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of CNN in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's build a simple CNN network for classification of color images using PyTorch. \n",
    "\n",
    "We will assume that the input image has only three channels (RGB). The first convolutional layer will take a one channel image and produce four different feature maps — like applying four different filters for the same image. Then we will apply an activation function and pool the features using 2x2 max pooling layer. \n",
    "\n",
    "Because we will also add padding, convolutional layer will produce feature map of the same size as the original images. But after pooling layer it will become twice smaller. If the original images has size of 256x256 pixels, after pooling we will get 4 feature maps with 128x128 pixels each. \n",
    "\n",
    "After that we add the second convolutional layer which will apply its filters to the the feature maps produced and pooled at the previous steps. It will take the four maps from the previous layer and will produce eight new feature maps as the result. \n",
    "\n",
    "The maps will also be pooled, so after pooling we will get 8 feature maps with 64x64 pixels each (for an image of size 256x256), which gives a vector with 32,768 feature values, which must be flattened and sent to the fully connected layers.\n",
    "\n",
    "Here is the full code of our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, width, height):\n",
    "        super().__init__()\n",
    "\n",
    "        # first convolutional layer\n",
    "        # - takes 3 channel image (RGB) and produces 4 channels (maps) with features\n",
    "        # - it uses kernel of size 3x3 and adds a pad of 1 pixel around to keep the same size\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, stride=1, padding=1)\n",
    "        # max pooling layer\n",
    "        # - has size of 2x2 so it reduces size of feature map twice\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # second convolutional layer\n",
    "        # - takes 4 channels (feature maps) and produce 8 channels with features\n",
    "        # - it uses kernel of size 3x3 and adds a pad of 1 pixel around to keep the same size\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)         # convolutional layer\n",
    "\n",
    "        # set of fully connected layers, number of inputs in this case depends on width\n",
    "        # and height of the original image (both will be reduced by 4)\n",
    "        self.fc1 = nn.Linear(8 * width // 4 * height // 4, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        # the output layer in this case has two outputs — one for each class\n",
    "        # the classification decision will be made by taking the biggest of the\n",
    "        # two output values\n",
    "        self.fc3 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))    # send input to 1st convolutional layer and ReLU\n",
    "        x = self.pool(x)             # pool the feature maps from previous layer\n",
    "        x = F.relu(self.conv2(x))    # send pooled features to 2nd convolutional layer and ReLU\n",
    "        x = self.pool(x)             # pool the feature maps again\n",
    "\n",
    "        x = torch.flatten(x, 1)      # flatten the outputs from the previous layer\n",
    "        x = F.relu(self.fc1(x))      # send flattened features to the 1st linear layer + ReLU\n",
    "        x = F.relu(self.fc2(x))      # send output of the 1st layer to the 2nd linear layer + ReLU\n",
    "        y_hat = self.fc3(x)          # send output of the 2nd layer to the output layer\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the model for images of 256x256 pixels size and look at the summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = ImageClassifier(256, 256)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the first convolutional layer should have 12 kernels 3x3 each (one kernel for each input and each output). Plus four bias values which gives: $3 \\times 4 \\times 3 \\times 3 + 4 = 112$ parameters.\n",
    "\n",
    "For the second we have: $4 \\times 8 \\times 3 \\times 3 + 8 = 296$ parameters.\n",
    "\n",
    "And so on, in total the model has more that 30 millions parameters to train! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's learn how we can use the CNN network, we have just defined, for real dataset. \n",
    "\n",
    "\n",
    "### Load images as dataset\n",
    "\n",
    "First of all, it worth to mention, that PyTorch has a special additional library, `torchvision` which helps to load images as datasets, assign them labels, etc. This library also provides a module `transform` which can apply various transformations, like we did in the first class: crop, resize, etc.\n",
    "\n",
    "For CNN it is important that:\n",
    "\n",
    "1. All images have the same size (same number of pixels).\n",
    "2. All images have the same color model or grayscale format.\n",
    "3. Pixels have intensity between 0 and 1.\n",
    "4. All images are PyTorch tensors.\n",
    "\n",
    "All these can be achieved by defining a sequence of transformation methods from `torchvision.transform` module, which Torch will automatically apply to each image.\n",
    "\n",
    "Let's look at the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# define same size for all images\n",
    "img_width = 256\n",
    "img_height = 256\n",
    "\n",
    "# define path to folder with images for each subset\n",
    "image_path = \"dataset\"\n",
    "\n",
    "# transformation sequence\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize([img_width, img_height]), # resize so each image has the same size\n",
    "    transforms.ToTensor()                       # convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "# create a dataset based on the image folder structure and defined transformation\n",
    "dataset = datasets.ImageFolder(root=image_path, transform=transform)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load two modules from `torchvision`. Module `dataset` contains methods which can load images from disk, assign labels, transform them to Torch tensors and combine into a dataset. Module `transform`, as mentioned already, contains methods for transformation of images.\n",
    "\n",
    "Then we define width and height of the target images in pixels. All images will be rescaled to this size. After that, we define the location of the images on disk (path to folder).\n",
    "\n",
    "If you open folder `dataset` you can see that inside this folder there are two others. Folder `glasses` contains images of faces with glasses and folder `noglasses` contains the face images without glasses. Check several images from each folder.\n",
    "\n",
    "Method `ImageFolder` that we use to load the images \"knows\" about this. So it will assign all images from the first subfolder class label `\"glasses\"` and to all images from the second subfolder class label `\"no glasses\"`. \n",
    "\n",
    "Let's investigate the dataset a little more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows list of classes\n",
    "dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric labels for each class\n",
    "dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show number of elements in the dataset\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of first 10 images\n",
    "dataset.samples[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, inside the `dataset` we do not have tensors with image pixels, but just a full path to every image and a numeric label, which is connected to the text class label. The images will be loaded during training and prediction process. This approach helps to save your computer memory.\n",
    "\n",
    "Let's show some of the images using PIL library we learned in the first class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_indices = range(0, 4000, 200)\n",
    "list(img_indices)\n",
    "list(range(len(img_indices)))\n",
    "\n",
    "img_indices[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# take every 200th of the first 4000 images\n",
    "img_indices = range(0, 4000, 200)\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "for i in range(len(img_indices)):\n",
    "    path, class_ind = dataset.samples[img_indices[i]]\n",
    "    img = Image.open(path)\n",
    "\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(np.array(img))\n",
    "    plt.axis('off')\n",
    "    plt.title(dataset.classes[class_ind])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's split the whole dataset to training, validation and test set. This time we will do it randomly, using function `random_split` from Torch. \n",
    "\n",
    "Ideal split will be to take 70% for training, 20% for validation and 10% for test. But our dataset is huge (4000+ images) and using even 70% of it for training will make this process very long until we run in on a powerful computer with GPU. \n",
    "\n",
    "To save time we will take only 20% (800+) of images for training , 10% for validation and 10% for testing. However, because `random_split` requires all percents to sum up to 100% we will create the fourth subset, `rest_set` which we will simply not use.\n",
    "\n",
    "Later, when you learn all the content try to increase the training set and see how it affects the model quality. In general the mode data you have the more efficient the training process will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nall = len(dataset)\n",
    "ntrain = int(nall * 0.20)\n",
    "ntrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "nall = len(dataset)\n",
    "ntrain = int(nall * 0.20)\n",
    "nval = int(nall * 0.10)\n",
    "ntest = int(nall * 0.10)\n",
    "nrest = nall - ntrain - nval - ntest\n",
    "\n",
    "\n",
    "# we need a seed here as well because of random splits\n",
    "torch.manual_seed(12)\n",
    "[train_set, val_set, test_set, rest_set] = random_split(dataset, (ntrain, nval, ntest, nrest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN model\n",
    "\n",
    "Training process for CNN model is almost identical to the one we used in the previous class. We will introduce two differences though.\n",
    "\n",
    "First important difference is that we will not send all images at once to the training process. It would take a lot of memory and computational power. Instead, we will do it by small portions — *batches*. \n",
    "\n",
    "So we will split all training and validations sets into batches and make a loop, so it takes images from the first batch, trains the model using this batch. Then takes images from the second batch, trains the model with the second batch and so on.\n",
    "\n",
    "This way of training is more efficient and is also a bit faster. Speed is important because in this case our model is very large and images contain a lot of pixels, so the training process will be much slower than in case of Iris classification.\n",
    "\n",
    "In order to use batches PyTorch has a special class, which is called `DataLoader`. So let's create loaders for the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data loader class\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# define how many images will be in one batch\n",
    "batch_size = 10\n",
    "\n",
    "# create data loaders with this batch size\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is a special parameter, `shuffle`, which is set to `True` for the training set loader. This means that images will be sorted into batches randomly. This is very important as it helps to avoid situations when, for example, all images in one batch contains faces without glasses, so the model can not learn anything from such batch.\n",
    "\n",
    "The batch size is another (together with learning rate) important setting which can influence the training quality, so it makes sense to vary it a little if the quality of the trained model is not satisfactory. \n",
    "\n",
    "These settings, which can not be optimized automatically, and its your responsibility to find the good ones, are called *hyperparameters*. So any model has *parameters* (such as weights and biases of the neurons), which are estimated automatically during the learning process, and *hyperparameters* (such as learning rate, batch size, number of epochs, etc.) which must be optimized manually by data scientist.\n",
    "\n",
    "Here is a code which implements batch learning (we also make it as a function like in the previous class). We use the same optimizer and same loss function like in the final example with Irises. The process will take up to 10-15 minutes, so it can be a good idea to start it, makes sure it works and take a break:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, nepochs = 100, lr = 0.001):\n",
    "    \"\"\" trains CNN model with provided data \"\"\"\n",
    "\n",
    "    # define a loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # define optimizer which will compute gradients — do the learning.\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # prepare arrays for losses\n",
    "    train_losses = np.zeros(nepochs)\n",
    "    val_losses = np.zeros(nepochs)\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(nepochs):  # Number of training epochs\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_data in train_loader:\n",
    "            inputs, labels = batch_data\n",
    "            optimizer.zero_grad()\n",
    "            labels_predicted = model(inputs)\n",
    "            loss = loss_function(labels_predicted, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + loss.item()\n",
    "        train_losses[epoch] = train_loss / len(train_loader)\n",
    "\n",
    "        # validate\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        for batch_data in val_loader:\n",
    "            inputs, labels = batch_data\n",
    "            labels_predicted = model(inputs)\n",
    "            loss = loss_function(labels_predicted, labels)\n",
    "            val_loss = val_loss + loss.item()\n",
    "        val_losses[epoch] = val_loss / len(val_loader)\n",
    "\n",
    "        # show how big the losses are at this epoch\n",
    "        print(f'Epoch {epoch}, train loss: {train_losses[epoch]:.4f} - validation loss {val_losses[epoch]:.4f}')\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed the random numbers generator to get reproducible results\n",
    "torch.manual_seed(12)\n",
    "\n",
    "# initialize the model\n",
    "model = ImageClassifier(img_width, img_height)\n",
    "\n",
    "# train it for 40 epochs and learning rate of 0.01\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader, nepochs = 40, lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the losses values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show plot with losses\n",
    "plt.plot(train_losses, label = \"train\")\n",
    "plt.plot(val_losses, label = \"val\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He we have several problems. First of all, you can see a strange behavior of the validation loss, it jumps up and down. Perhaps we need to use a smaller learning rate, we will find this out later. \n",
    "\n",
    "Second problem is that starting from approximately 20th epoch the validation loss is going slowly up, so the final model is not the most optimal one. Let's talk about how to handle this problem also a bit later but now let's check the performance of the model on the test set.\n",
    "\n",
    "To do this we need to write a new function for making predictions. As you remember, this time we use datasets based on image folder. This types of datasets contain path to the images and corresponding labels and then use data loaders to load the images from disk, preprocess them, split them to batches and feed the model with the batches.\n",
    "\n",
    "For making predictions we do not need batches, but using data loader is still handy as it automatizes a lot of things. What we can do is to create a loader with a single batch, so all images will be in that batch and use it to make predictions. \n",
    "\n",
    "Here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset):\n",
    "    \"\"\" get ANN model and tensor with predictors and returns predicted class label indices \"\"\"\n",
    "    model.eval()\n",
    "    data_loader = DataLoader(dataset, batch_size = len(dataset))\n",
    "    for inputs, labels in data_loader:\n",
    "        output = model.forward(inputs)\n",
    "        _, predicted_labels = torch.max(output, 1)\n",
    "\n",
    "    return labels, predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this function returns both labels and predicted labels, so we can easily reuse the other functions we have created in the last class to check the model performance — computation of cross table and visualization of this table as heat map.\n",
    "\n",
    "Let's make predictions first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels, labels = predict(model, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = table(labels, predicted_labels)\n",
    "ct_heatmap(ct, dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No bad at all, right? Of course the result will vary if you comment the `manual_seed()` line and run it several times, because in this case the weights will be initialized randomly and every time you run the training process you will get different performance. \n",
    "\n",
    "Now you have a new achievement — you created and trained a CNN model, which can automatically recognize people with glasses. Similar models can do more useful job, for example detect if a car driver uses mobile phone while driving (you probably heard that this is illegal) or sorting different objects (for example sorting garbage, vegetables, or similar).\n",
    "\n",
    "There are still a couple of things to learn, but now it is time for exercise:\n",
    "\n",
    "### Exercise\n",
    "\n",
    "In the previous class you learned how to save a state dictionary of a model at any stage to a variable (by taking a deep copy of the dictionary). Modify the function `train_model()` in the code block below, so it always results in a model with the lowest validation loss.\n",
    "\n",
    "For example, if you run a model for 100 epochs and the lowest validation loss was obtained at epoch 67, the function will save the state of this model and at the end of the training loop will load this state to the current model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def train_model(model, train_loader, val_loader, nepochs = 100, lr = 0.001):\n",
    "    \"\"\" trains CNN model with provided data \"\"\"\n",
    "\n",
    "    # define a loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # define optimizer which will compute gradients — do the learning.\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # prepare arrays for losses\n",
    "    train_losses = np.zeros(nepochs)\n",
    "    val_losses = np.zeros(nepochs)\n",
    "\n",
    "    # HINT: here you need to initialize two variables\n",
    "    # - one will keep the best validation loss value\n",
    "    # - second one will keep the state dictionary of the model you got this loss for\n",
    "    best_model = None\n",
    "    best_loss = 99999999999.0\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(nepochs):  # Number of training epochs\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_data in train_loader:\n",
    "            inputs, labels = batch_data\n",
    "            optimizer.zero_grad()\n",
    "            labels_predicted = model(inputs)\n",
    "            loss = loss_function(labels_predicted, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + loss.item()\n",
    "        train_losses[epoch] = train_loss / len(train_loader)\n",
    "\n",
    "        # validate\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        for batch_data in val_loader:\n",
    "            inputs, labels = batch_data\n",
    "            labels_predicted = model(inputs)\n",
    "            loss = loss_function(labels_predicted, labels)\n",
    "            val_loss = val_loss + loss.item()\n",
    "        val_losses[epoch] = val_loss / len(val_loader)\n",
    "\n",
    "        # show how big the losses are at this epoch\n",
    "        print(f'Epoch {epoch}, train loss: {train_losses[epoch]:.4f} - validation loss {val_losses[epoch]:.4f}')\n",
    "\n",
    "        # HINT:\n",
    "        # here you need to add a condition which will compare current model with the\n",
    "        # best one you got so far. If the current model is better, you save its state as\n",
    "        # the new best. You also need to save the best loss value — this is the way to\n",
    "        # see if the next model will be even better\n",
    "        if val_losses[epoch] < best_loss:\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "            best_loss = val_losses[epoch]\n",
    "\n",
    "    # HINT:\n",
    "    # here you need to load the state of the best model from the loop\n",
    "    # to your model object\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def train_model(model, train_loader, val_loader, nepochs = 100, lr = 0.001):\n",
    "    \"\"\" trains CNN model with provided data \"\"\"\n",
    "\n",
    "    # define a loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # define optimizer which will compute gradients — do the learning.\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # prepare arrays for losses\n",
    "    train_losses = np.zeros(nepochs)\n",
    "    val_losses = np.zeros(nepochs)\n",
    "\n",
    "    # initialize two variables which will keep the best validation loss and the best model state\n",
    "    best_loss = 999999999999\n",
    "    best_model_state = None\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(nepochs):  # Number of training epochs\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_data in train_loader:\n",
    "            inputs, labels = batch_data\n",
    "            optimizer.zero_grad()\n",
    "            labels_predicted = model(inputs)\n",
    "            loss = loss_function(labels_predicted, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + loss.item()\n",
    "        train_losses[epoch] = train_loss / len(train_loader)\n",
    "\n",
    "        # validate\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        for batch_data in val_loader:\n",
    "            inputs, labels = batch_data\n",
    "            labels_predicted = model(inputs)\n",
    "            loss = loss_function(labels_predicted, labels)\n",
    "            val_loss = val_loss + loss.item()\n",
    "        val_losses[epoch] = val_loss / len(val_loader)\n",
    "\n",
    "        # show how big the losses are at this epoch\n",
    "        print(f'Epoch {epoch}, train loss: {train_losses[epoch]:.4f} - validation loss {val_losses[epoch]:.4f}')\n",
    "\n",
    "        # check if validation loss is better to what is known so far\n",
    "        # if so save the model state\n",
    "        if val_losses[epoch] < best_loss:\n",
    "            best_loss = val_losses[epoch]\n",
    "            best_model_state = deepcopy(model.state_dict())\n",
    "\n",
    "    # load the state from the best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you write your function, test it using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed the random numbers generator to get reproducible results\n",
    "torch.manual_seed(12)\n",
    "\n",
    "# initialize the model\n",
    "model = ImageClassifier(img_width, img_height)\n",
    "\n",
    "# train it for 40 epochs and learning rate of 0.01\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader, nepochs = 40, lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute loss of the final model on validation set\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "val_loss = 0\n",
    "model.eval()\n",
    "for batch_data in val_loader:\n",
    "    inputs, labels = batch_data\n",
    "    labels_predicted = model(inputs)\n",
    "    loss = loss_function(labels_predicted, labels)\n",
    "    val_loss = val_loss + loss.item()\n",
    "val_loss = val_loss / len(val_loader)\n",
    "\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see that the loss we got in this test is smaller than the loss shown for the last epoch when we trained the model, it works. \n",
    "\n",
    "Let's visualize this by showing a plot with losses from the training process and the loss of the final model as horizontal line. If the line touches the validation loss curve in the global minimum, your function works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show plot with losses\n",
    "plt.plot(train_losses, label = \"train\")\n",
    "plt.plot(val_losses, label = \"val\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# show the recently computed loss as horizontal line\n",
    "plt.plot(plt.xlim(), [val_loss, val_loss], color = \"black\", linestyle = \"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions for new images\n",
    "\n",
    "What if we want to make a prediction for a new image? Just one single image whose real label we do not know, so we can not use the data loader in this case. Let's take the image number 2000 from the original dataset set for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset items contain two elements - path and label, so we take the first one\n",
    "new_image_path = dataset.imgs[2000][0]\n",
    "new_image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(new_image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the person wears glasses, let's see how to make predictions without data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually apply all transformations to the new image\n",
    "img_transformed = transform(img)\n",
    "\n",
    "# reshape image tensor because model does not work with single images\n",
    "# it expects tensor of images. So if we need to feed model the image\n",
    "# we make it a tensor with one image\n",
    "img_transformed = img_transformed.reshape(1, 3, img_width, img_height)\n",
    "\n",
    "# apply the model\n",
    "output = model.forward(img_transformed)\n",
    "\n",
    "# compute label\n",
    "_, labels_predicted = torch.max(output, 1)\n",
    "\n",
    "# show all outcomes\n",
    "(output, labels_predicted, dataset.classes[labels_predicted])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply network to camera image\n",
    "\n",
    "Now let's learn how we can use the model in order to make predictions in real time, by taking photos with you frontal camera. Run the next code in order to get the image of your face or a face of your friend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to camera\n",
    "import cv2\n",
    "camera = cv2.VideoCapture(1)\n",
    "camera.isOpened()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a picture (repeat if necessary to get better result)\n",
    "ret, frame = camera.read()\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close connection to the camera\n",
    "camera.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the coordinate axes in order to define a crop rectangle. Make sure the final face is located at the center of the cropped image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left top right bottom\n",
    "crop_rect = [650, 0, 1450, 1100]\n",
    "img = Image.fromarray(frame)\n",
    "img_cropped = img.crop(crop_rect)\n",
    "display(img_cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the model and see prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transformed = transform(img_cropped)\n",
    "img_transformed = img_transformed.reshape(1, 3, 224, 224)\n",
    "output = model.forward(img_transformed)\n",
    "\n",
    "# compute label\n",
    "_, labels_predicted = torch.max(output, 1)\n",
    "\n",
    "# show all outcomes\n",
    "(output, labels_predicted, dataset.classes[labels_predicted])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do calculations on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You may noticed that in contrast to experiments with Iris data, training and using model for images require much longer time. This process can be speed up if you have NVIDIA GPU on your computer (even simplest one, like RTX 1060).\n",
    "\n",
    "In order to do this you need to send your model and your data to the GPU device using a special method. Below you will find a code which implements this approach. The code is versatile, which means if you have GPU it will use GPU but if not, you can run your code on CPU without changing anything.\n",
    "\n",
    "First of all let's learn how to detect the compatible GPU device. In terms of Torch such device is called [CUDA](https://en.wikipedia.org/wiki/CUDA) (`cuda`), it is a name of corresponding computer library which lets use GPU device for calculations. Here is how to check if you have one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this code will automatically define the available device and keep it in separate variable. If you have CUDA it will select it, if not it will select you CPU instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's re-write the training loop in order to use the detected device for training. Here we reuse data loaders and model class you have created before in this class, so to make it work you should run the previous code cells.\n",
    "\n",
    "The code is kept as simple as possible without validation loop, just to give you an idea. As you can see, there are only two changes, one line where we send the model to the device (`model.to(device)`). And second line where we do the same with the batch data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model\n",
    "model = ImageClassifier(img_width, img_height)\n",
    "\n",
    "# define number of epochs and learning rate\n",
    "nepochs = 30\n",
    "lr = 0.01\n",
    "\n",
    "# define a loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# define optimizer which will compute gradients — do the learning.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# NEW: send the model to device you defined earlier\n",
    "model.to(device)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(nepochs):  # Number of training epochs\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_data in train_loader:\n",
    "        inputs, labels = batch_data\n",
    "\n",
    "        # NEW: send batch data to the device as well\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        labels_predicted = model(inputs)\n",
    "        loss = loss_function(labels_predicted, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = train_loss + loss.item()\n",
    "\n",
    "    train_losses = train_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch}, train loss: {train_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you train your model om GPU and want to make predictions, the new data (test set, validation set or new image) must be also loaded to GPU first. Alternatively you can unload your model from GPU/CUDA to CPU by running: `model.to(torch.device(\"cpu\"))` after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "\n",
    "As you may noticed, the training process, even for relatively simple network as we created on relatively small dataset, takes a long time. Using GPU/CUDA can solve this problem but only partially. Good models with high accuracy are much more complicated and have been trained on much larger dataset. Can we use benefits of the model trained by someone else?\n",
    "\n",
    "Yes, we can and this is exactly what *transfer learning* does. The idea is that you take a model trained already on a very broad range of images and image classes. And then you fine tune this model, to make it work on your particular dataset.\n",
    "\n",
    "Because weights of this model are already set, you do not need to start learning process from the scratch and this saves a lot of resources.\n",
    "\n",
    "Torch has already several pre-treated models, including famoues ones like [AlexNet](https://en.wikipedia.org/wiki/AlexNet), [VGG](https://www.kaggle.com/code/blurredmachine/vggnet-16-architecture-a-complete-guide) and many others. Models for image classification are located in `torchvision.models`. Let's load one of those ([residual network](https://en.wikipedia.org/wiki/Residual_neural_network) with 18 layers, there is also one with 50):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchinfo import summary\n",
    "\n",
    "model = models.resnet18()\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model has less parameters than our but the architecture is much more complex, which makes it more efficient and versatile (the output is actually truncated so you do not see the full structure).\n",
    "\n",
    "In order to use the network we need to know how to transform (preprocess) the images and what should be the image size. Luckily we can get the already prepared stack of transformations, which is connected to the weights of the model. \n",
    "\n",
    "Here is how to get it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = models.ResNet18_Weights.DEFAULT\n",
    "transform = weights.transforms()\n",
    "transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the transformation stack is different from what we used before, we need to recreate the dataset, subsets and data loaders. We just repeat the code with new transformation object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset based on the image folder structure and defined transformation\n",
    "dataset = datasets.ImageFolder(root=image_path, transform=transform)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "nall = len(dataset)\n",
    "ntrain = int(nall * 0.20)\n",
    "nval = int(nall * 0.10)\n",
    "ntest = int(nall * 0.10)\n",
    "nrest = nall - ntrain - nval - ntest\n",
    "\n",
    "\n",
    "torch.manual_seed(12)\n",
    "\n",
    "[train_set, val_set, test_set, rest_set] = random_split(dataset, (ntrain, nval, ntest, nrest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data loader class\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# define how many images will be in one batch\n",
    "batch_size = 10\n",
    "\n",
    "# create data loaders with this batch size\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training we actually need to modify the model. The original model can classify images among 1000 classes, so it has 1000 outputs. You can see this if you check the output layer, which has name `fc` in this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can adjust it by creating a new linear layer with as many inputs as the original one and only two outputs. We can of replace part of the ResNet18 model. Here is how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the output layer in the ResNet18 model by a new one with 2 outputs\n",
    "in_features = model.fc. in_features\n",
    "model.fc = nn.Linear(in_features, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we ready to train, or rather fine tune the model. We will start with just 10 epochs. If you created the smart `train_model()` method in one of the exercises below it will end up with the most optimal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = train_model(model, train_loader, val_loader, nepochs=10, lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, already after 1-2 epochs the loss gets very small. Let's check the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, predicted_labels = predict(model, test_set)\n",
    "ct = table(labels, predicted_labels)\n",
    "ct_heatmap(ct, dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect result and much faster than training the CNN model from the scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do next?\n",
    "\n",
    "If you want to continue and develop your skills further, here are some useful links.\n",
    "\n",
    "To learn Python in a more systematical way, you can look into the following materials:\n",
    "\n",
    "* [Python for kids](https://www.geeksforgeeks.org/python-for-kids/)\n",
    "* [Google Python class](https://developers.google.com/edu/python)\n",
    "* [Python tutorial at geeksforgeeks.org](https://www.geeksforgeeks.org/python-programming-language-tutorial/)\n",
    "* [Scientific Python lectures](https://lectures.scientific-python.org/)\n",
    "\n",
    "There are tones more of course, Python is probably the most popular programming language nowadays.\n",
    "\n",
    "As for the data science, machine learning or artificial intelligence, there are hundreds of good courses as well. We recommend a [free online course](https://course.fast.ai) from FastAI which covers all aspects of modern ML/AI including more sophisticated topics like Stable Diffusion. It is also based on Jupyter notebooks, so you will fell comfortable from the start.\n",
    "\n",
    "Happy learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
