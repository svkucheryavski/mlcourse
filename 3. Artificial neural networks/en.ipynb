{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Artificial Neural Networks \n",
    "\n",
    "Artificial Neural Networks (ANNs) provide a versatile way to build any machine learning model. The idea is inspired by the structure and functioning of the human brain. Neural networks have proven to be powerful for solving complex problems of classification and other tasks. In this class, we will explain the basics of neural networks, their components, training process, and validation of ANNs.\n",
    "\n",
    "There are several libraries for Python which implement ANN. The most known are [Tensorflow](https://www.tensorflow.org) and [PyTorch](https://pytorch.org) (there is also a library which takes the best of both, [Keras](https://keras.io)). For the examples in this class, we will use PyTorch, which is, these days, probably the most widely used and well documented. \n",
    "\n",
    "Install the library by running the following code (after that, add `#` in front of this line to avoid running it again):\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may noticed we installed two libraries here, the PyTorch (`torch`) and a supplement which will help us to discover the structure of ANNs (`torchinfo`).\n",
    "\n",
    "Before we continue, let's split our data to the training and test set again, as we did it in the previous class. We will later use the training set for the learning process and test set to assess how good the trained model is.\n",
    "\n",
    "Strictly speaking, in this case, we need three sets:\n",
    "\n",
    "* *training set* is used for learning\n",
    "* *validation set* is used to optimize the model (find the best one during learning)\n",
    "* *test set* is used to assess quality of the final model\n",
    "\n",
    "In order to simplify examples, we will use *test set* both for validation and testing, but when you work with real cases, make sure that they are separated and you have all three sets (we will use this approach in the next class).\n",
    "\n",
    "Like in the previous class, we simply take every fifth measurement as a test set, and later we will learn how to make it in a better way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from CSV file as data frame\n",
    "import pandas as pd\n",
    "d = pd.read_csv(\"Iris.csv\")\n",
    "\n",
    "# generate logical values for train and test set measurements (rows of data frame)\n",
    "train_ind = d[\"Id\"] % 5 != 0\n",
    "test_ind = d[\"Id\"] % 5 == 0\n",
    "\n",
    "# make the split\n",
    "d_train = d.loc[train_ind]\n",
    "d_test = d.loc[test_ind]\n",
    "\n",
    "# show size of each set\n",
    "(d_train.shape, d_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is simply a set of nodes, known as *neurones*, that are connected to each other. In the simplest case, a neural network consists of only one node, as shown in the image below.\n",
    "\n",
    "<img src=\"./illustrations/Neuron.png\" style=\"width:500px; height:400px;\"/>\n",
    "\n",
    "Every neuron has a set of inputs (shown as $X_1$, $X_2$, $X_3$, and $X_4$ on the left part of the image) and an output (shown as $\\hat{Y}$ on the right part). It does a very simple thing — takes all numbers, which it receives from the inputs, and applies a mathematical function, which computes an output value based on the inputs. \n",
    "\n",
    "In the simplest case, it computes a weighted sum (known as *linear combination*) of these numbers and transmits the computed number to the output. Mathematically, we can write this as follows:\n",
    "\n",
    "$\\hat{Y} = (X_{1}\\times W_{1}) + (X_{2}\\times W_{2}) + (X_{3}\\times W_{3}) + (X_{4}\\times W_{4}) + Bias$\n",
    "\n",
    "The values $W_1$,...,$W_4$ are *weights* every input contributes to the output with.\n",
    "\n",
    "Imagine that the inputs are the measurements from the first row of the Iris dataset: \n",
    "\n",
    "$X = [5.1,3.5,1.4,0.2]$ \n",
    "\n",
    "and the weights are: \n",
    "\n",
    "$W = [0.1, 0.2, 2.0, 5.0]$. \n",
    "\n",
    "Let's assume that the bias is $1.0$. Then the output value for our neuron will be:\n",
    "\n",
    "$\\hat{Y} = 5.1 \\times 0.1 + 3.5 \\times 0.2 + 1.4 \\times 2.0 + 0.2 \\times 5.0 + 1.0 = 6.01$\n",
    "\n",
    "As simple as that.\n",
    "\n",
    "Here is how we can implement this one neuron based ANN in Python using PyTorch library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    \"\"\" class for one-neuron ANN model \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # initialize the parent (super) class for ANN model\n",
    "        super(SimpleModel, self).__init__()\n",
    "        # define all layers with neurons and their properties\n",
    "        self.layer1 = nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" takes vector with input values 'x', computes and returns the output \"\"\"\n",
    "        y_hat = self.layer1(x)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the code is a bit more complex comparing to what we have had before. In this code we create a *class* `SimpleModel` on top of another class `nn.Module`, which is already implemented in PyTorch. \n",
    "\n",
    ">You can think about class as a recipe, a blue print, a set of instructions. Imagine you want to build a house. There is a set of basic instructions which tell how to build a foundation of the house, set drainage pipes etc. Plus it has some typical instructions, e.g. how to make a brick wall. There is no need to reinvent the wheel, so when you want to make instructions for building a full house (a typical project), with walls, roof, etc., you can take the basic set as a basis and extend it with your own part. \n",
    "\n",
    "Same idea here, the `nn.Model` is a basic set of instructions how to make any neural network in PyTorch, and it includes a lot of things you do not need to care of. You simply take it as a basis and extend with your own specific parts — which nodes to use, how many inputs they have, how many outputs, etc. So this new set of instructions is the class `SimpleModel`.\n",
    "\n",
    "As you can see, we added two methods to the class. \n",
    "\n",
    "The first method, `__init__` is needed to initialize your model. At the beginning it contains:\n",
    "\n",
    "```python\n",
    "super(SimpleModel, self).__init__()\n",
    "``` \n",
    "\n",
    "which tells Python to make all preparations based on the basic set of instructions (e.g. make foundation of the house).\n",
    "\n",
    "And then you define your network — how many neurons, what kind of neurons and define their properties. In this case, we have one linear neuron with 4 inputs and 1 output — exactly what we used in the example above. \n",
    "\n",
    "**You can think of `__init__()` as a method that builds the house using your instructions.** Not painted, without furniture, but a whole, fully functioning house. \n",
    "\n",
    "The second method in this class, `forward()`, is used every time you want to apply your model for given inputs to produce the output. The method \"connects\" the neurons, it makes sure that the inputs go through the neurons in a correct order and returns the resulted output at the end.  \n",
    "\n",
    "**You can think of `forward()` as a method that lets you use the house you have created.** It is always used in its current state. So if you apply method `forward()` to newly created house, it will not perform well as your house has bare walls and no furniture. But you can live there of course.\n",
    "\n",
    "Here is an example how we can use the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = SimpleModel()\n",
    "\n",
    "# define values for input\n",
    "X = torch.tensor([[5.1, 3.5, 1.4, 0.2]])\n",
    "\n",
    "# send input to the model and get the output\n",
    "y_hat = model(X)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that every time we need to provide some numbers to PyTorch it is not enough just to combine them into a 1D list using the squared brackets, e.g.:\n",
    "\n",
    "```python\n",
    "X = [5.1, 3.5, 1.4, 0.2] \n",
    "```\n",
    "\n",
    "or make 2D list like:\n",
    "\n",
    "```python\n",
    "X = [[5.1, 3.5, 1.4, 0.2]]\n",
    "```\n",
    "\n",
    "We also need to convert the list to a special type, `torch.tensor`. This is similar to what we used in the first class to create NumPy arrays, we provided values as a list and then used a special method that tells Python — make it NumPy array:\n",
    "\n",
    "```python\n",
    "X = np.array([[5.1, 3.5, 1.4, 0.2]])\n",
    "```\n",
    "\n",
    "From visual point of view *Tensor* is the same as array, it can be 1D (vector), 2D (matrix), 3D and so on. So, Torch tensor is similar to NumPy array you already know. But in reality there is a difference. For Python they are two different data objects from two different libraries, and you can apply different methods and operators to each. Therefore it is important to \"tell\" Python that this is not just a list or NumPy array, but a PyTorch tensor.\n",
    "\n",
    ">One of the examples here can be cars and service centers. If you have Ford, you will not go to Hyundai's service center to e.g. change breaks and engine oil. Because their mechanics are not certified to work with this brand. Although both are cars, they look similar, work similar, and you can easily drive both, they are not identical.  \n",
    "\n",
    "One of the reasons to use Torch tensors instead of NumPy arrays is that the tensors are designed to work with [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit) — the powerful graphic cards we have e.g. in gaming computers. Moreover it works only with specific GPU, made by company [NVIDIA](https://www.nvidia.com/da-dk/geforce/graphics-cards/). If you are gamer you probably heard about graphics card like GTX 3090. Because ANNs require a lot of computer power, using GPU makes them much faster and PyTorch is made to work on GPU first. If you do not have GPU, it can also work on conventional processor, but the computation will be slow. Especially if you have large datasets with thousands of objects.\n",
    "\n",
    "So although it is a bit annoying that we have to add `torch.tensor()`, you will find out later that it is not a big problem.\n",
    "\n",
    "If we get back to our code and its computed value, you can see, the output is not $6.01$, more over every time you run this code (try to click on *Run* button several times), you will get a new output. Because when you initialize the ANN model it uses random numbers for your weights. \n",
    "\n",
    "You can change this and assign weights manually after initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define weights and bias\n",
    "W = torch.tensor([[0.1, 0.2, 2.0, 5.0]])\n",
    "Bias = torch.tensor([1.0])\n",
    "\n",
    "# set weights and bias manually for the neurons on layer \"layer1\"\n",
    "model.layer1.weight = nn.Parameter(W)\n",
    "model.layer1.bias = nn.Parameter(Bias)\n",
    "\n",
    "# forward pass through the model\n",
    "y_hat = model(X)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is $6.01$! You can notice that the value we get as an output is also a tensor.\n",
    "\n",
    "By the way, with Torch, you can also compute linear combinations manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X * W).sum() + Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's now implement the classification rule for *Virginica* samples we created in a previous class. If you remember we compared Petal Width ($X_4$ in our case)  of a flower with 1.7 and if the value was above this threshold we classified this flower as *virginica*. \n",
    "\n",
    "These are the weights and bias which can implement this rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define weights and bias\n",
    "W = torch.tensor([[0.0, 0.0, 0.0, 1.0]])\n",
    "Bias = torch.tensor([-1.7])\n",
    "\n",
    "# set weights and bias manually\n",
    "model.layer1.weight = nn.Parameter(W)\n",
    "model.layer1.bias = nn.Parameter(Bias)\n",
    "\n",
    "# forward pass through the model\n",
    "y_hat = model(X)\n",
    "\n",
    "# show result and apply threshold\n",
    "(y_hat, y_hat > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all weights except the last one are set to zero, while the weight for $X_4$ is equal to one. This gives the output, $\\hat{Y}$ simply equal to the $X_4$ value, which in our case is Petal Width. After that, we apply bias, so we subtract our threshold. This gives the following:\n",
    "\n",
    "$\\hat{Y} = X_4 - 1.7$\n",
    "\n",
    "Apparently, if *Petal Width* is below the threshold, the output value will be negative, and when it is above, the value will be positive. So we can make a classification decision by simply comparing it with 0, like we did above.\n",
    "\n",
    "Now let's apply this network with manually set weights to the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only columns with measurements and convert them to Torch tensor\n",
    "X_test_values = d_test.iloc[:, 1:5].values\n",
    "X_test = torch.tensor(X_test_values).float()\n",
    "\n",
    "# apply the model\n",
    "y_hat = model(X_test)\n",
    "\n",
    "# show the results\n",
    "y_hat > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we got a lot of negative numbers at the beginning, where we had flowers of *Setosa* and *Versicolor* species, and positive numbers at the end, where we had flowers of the target class, *Virginica*.\n",
    "\n",
    "Let's combine the output and the reference values into a data frame for better visibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert output to numpy array (transpose -> detach from model -> convert to NumPy array)\n",
    "y_hat_arr = y_hat.t().detach().numpy()\n",
    "y_hat_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# combine with reference values\n",
    "res = pd.DataFrame({\n",
    "    \"Reference\": d_test[\"Species\"],\n",
    "    \"Predicted\": y_hat_arr[0],\n",
    "    \"Is virginica\": y_hat_arr[0] > 0\n",
    "})\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got absolutely the same results as in the last class! \n",
    "\n",
    "But how to let the model find the classification decision automatically, based on the provided data? We need to train the model! But what we should use as a training criterion?\n",
    "\n",
    "## Loss function\n",
    "\n",
    "The whole idea of ANN training is to find the weights (and biases and other parameters, if any) for all neurons which will make the output as close to the desired as possible. \n",
    "\n",
    "But how to measure the distance between the output of the model and the desired output? This is what is defined as a *loss*. Loss is a number, a statistic, which tells how big the difference is between the desired output, $Y$, and the predicted one, $\\hat{Y}$.\n",
    "\n",
    "As you remember, we want to make a classification for *Virginica* flowers. Let's define the ideal output to be 1 for *virginica* and -1 for the others. Here's how to create it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the classes labels for the training set\n",
    "c = d_test[\"Species\"]\n",
    "\n",
    "# compare the labels with target class then multiply the result to 2 and subtract 1\n",
    "# if the result is False, it will be treated as 0: 0 * 2 - 1 = -1\n",
    "# if the result is True, it will be treated as 1: 1 * 2 - 1 = 1\n",
    "y_dummy = (c == \"virginica\") * 2 - 1\n",
    "y_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the values from the data frame column to torch array\n",
    "y = torch.tensor([y_dummy.values])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only problem we have is that the values are presented as rows and we need them as a column. So let's add transposition and also convert them from integer number to floating point numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the values to torch array with 1 column and 120 rows\n",
    "y = torch.tensor([y_dummy.values]).float().t()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time we get output from the model we need to compare it with these reference y-values and compute a statistic which tells how big the difference is — the *loss*. The simplest statistic which will do it for you is called mean squared error (MSE). In this case you simply take a difference between the two vector of values, square this difference and compute average (mean). \n",
    "\n",
    "Here is how to implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_hat, y):\n",
    "    \"\"\" computes mean squared error loss \"\"\"\n",
    "    return ((y - y_hat)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test it. We already have the outputs and the desired (reference) values for the test set, so we can compute the loss value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test = mse_loss(y_hat, y)\n",
    "loss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smaller loss the better. \n",
    "\n",
    "In fact you do not need to calculate the loss manually, like we did in the code block above. PyTorch has a lot of loss functions already implemented and specifically created to be used in the training process (e.g. for computing gradients). Here is the one for MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "loss_test = loss_function(y_hat, y)\n",
    "\n",
    "loss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it provides value identical to what we got using our own manual implementation of the MSE loss.\n",
    "\n",
    "As we mentioned above, there are many different ways to measure the loss, so MSE is not the only one which is in use. For example, in case of binary classification another function, *Binary Correlation Entropy* (BCE) is used. In case of multiple class classification, one can use *Cross Entropy Loss*.\n",
    "\n",
    "You do not need to know all of them, just remember that the loss function shows you (and your model) how big the difference is between the desired output and the ones you model computes now. \n",
    "\n",
    "Moreover, it tells the model how to compute gradients —a set of steps which let ANN change the weights in order to make the loss smaller. This process of reducing the loss by gradually updating weights is called *[gradient descent](https://www.ibm.com/topics/gradient-descent)*, and this is the main way to train any ANN model. Now we can discuss the training in detail.\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Fill in the following two tables (you can do it in e.g. Excel or manually on paper), compute MSE for each, and comment on how well MSE describes the classification results:\n",
    "\n",
    "\n",
    "*Case 1*\n",
    "\n",
    "| $y$ | $\\hat{y}$ | $(y - \\hat{y})$ | $(y - \\hat{y})^2$ |\n",
    "| --:| ---------:| ---------------:| -----------------:|\n",
    "| -1 | 0.2 | - | - |\n",
    "| -1 | 0.4 | - | - |\n",
    "|  1 | -0.2 | - | - |\n",
    "|  1 | -0.4 | - | - |\n",
    "\n",
    "*Case 2*\n",
    "\n",
    "| $y$ | $\\hat{y}$ | $(y - \\hat{y})$ | $(y - \\hat{y})^2$ |\n",
    "| --:| ---------:| ---------------:| -----------------:|\n",
    "|  1 | 0.2 | - | - |\n",
    "|  1 | 0.4 | - | - |\n",
    "| -1 | -0.2 | - | - |\n",
    "| -1 | -0.4 | - | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients and optimization \n",
    "\n",
    "The loss function is used not only for assessing the quality of the predicted values, but also for computing gradients for the weights —how the weights must be changed in order to get a smaller loss at the next iteration (improve the model). \n",
    "\n",
    "The gradients are increments for the weights, $\\Delta w_1, \\Delta w_2, \\Delta w_3, \\Delta w_4$ and for the bias, $\\Delta b$, which are used to compute new weights for the model. The simplest way to compute the new weights is the following:\n",
    "\n",
    "\n",
    "$w_1 = w_1 - \\alpha \\Delta w_1$<br>\n",
    "$w_2 = w_2 - \\alpha \\Delta w_2$<br>\n",
    "$w_3 = w_3 - \\alpha \\Delta w_3$<br>\n",
    "$w_4 = w_4 - \\alpha \\Delta w_4$<br>\n",
    "$bias = bias - \\alpha \\Delta b$\n",
    "\n",
    "As you can see above, the increments are not used as is, but there is an additional parameter $\\alpha$, which must be between 0 and 1. For example, if $\\alpha = 0.01$, the change in weights  will be 1% of the computed increment.\n",
    "\n",
    "The parameter $\\alpha$ is called a *learning rate*, and it is needed to slow down the learning process and make it more smooth. You will see some examples later in this class.\n",
    "\n",
    "The gradients are computed by the loss function object (in the example above, it is `nn.MSELoss()`), while updating the weights is done by another function, *optimizer*. There are several optimizers available, usually, one of the following two is a good choice:\n",
    "\n",
    "* *GD* or *SGD* (stochastic gradient descent) is the simplest optimizer, which works exactly as shown in the equations above. Simple and straightforward. The name *gradient descent* means that you compute the gradient in order to find steps for the weights which let go down (descent) in the loss.\n",
    "* *Adam* is a more sophisticated optimizer which updates weights in a bit more complex way than shown above. It has more parameters to tune and is usually more efficient.\n",
    "\n",
    "In all examples, we will use *SGD* as it is more simple, but you can try *Adam* later. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training ANN model\n",
    "\n",
    "The idea of ANN training is as follows:\n",
    "\n",
    "1. Apply ANN to compute outputs based on inputs from the training set.\n",
    "2. Compute loss based on the current output and the desired output values.\n",
    "3. Compute gradients and update weights, so next time the loss value is smaller.\n",
    "\n",
    "The first step is called *forward propagation* because data values flow from the left (inputs) to the right (outputs) through all neurons in between (we have only one neuron so far in our model, but it does not matter — what works for one will work for thousands). \n",
    "\n",
    "The second step is the process of computing the difference between the predicted values and the actual values based on the intial values of weight and bias for each neuron. As previously mentioned, it is the loss that should be minimized.\n",
    "\n",
    "The third step is called *back propagation*, because the gradients are computed for the output neuron first. Then for the neurons left to the output, and so on, until all neurons get the new weights updated based on the gradients.\n",
    "\n",
    "These three steps are repeated until a criterion is met, for example loss does not get smaller anymore. Every time a model takes all three steps for all rows of the training set it takes an *epoch*. So if you run training process for 10 epochs, it means this three steps are repeated 10 times for all rows of the training set.\n",
    "\n",
    "Let's implement this for our model. First of all, let's prepare X and y values from our training set (so they are numeric and converted to Torch tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select X variables, convert them to torch tensor and make them to have type float\n",
    "X_train_values = d_train.iloc[:, 1:5].values\n",
    "X_train = torch.tensor(X_train_values).float()\n",
    "\n",
    "# compare species values with target class to get logical target values\n",
    "c_train = d_train[\"Species\"]\n",
    "\n",
    "# convert logical values to numeric, so it is +1 if flower is virginica and -1 otherwise\n",
    "y_train_dummy = (c_train  == \"virginica\") * 2.0 - 1.0\n",
    "\n",
    "# convert the numeric values to torch tensor, make them float and transpose to a column\n",
    "y_train = torch.tensor([y_train_dummy.values]).float().t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the number of epochs for training and the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "nepochs = 20\n",
    "\n",
    "\n",
    "# define a loss function\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many epochs to use depends on the case, usually, 100 epochs can be used as a starting point. We will use 20 just to make the output shorter.\n",
    "\n",
    "We are ready to train the model.\n",
    "\n",
    "Before we start, we will fix the state of the random number generator (which is used to initialize weights). This is needed to get reproducible results below, so when we re-run the code, the results will be the same. If you want later to try this code with truly random weights later, simply comment on these two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fix the state of random numbers generator\n",
    "seed = 11\n",
    "#torch.manual_seed(seed)\n",
    "\n",
    "# initialize a new model\n",
    "model = SimpleModel()\n",
    "\n",
    "# define optimizer which will compute gradients — do the learning, back propagation.\n",
    "#\n",
    "# parameter \"lr\" is learning rate it tells how large changes\n",
    "# the weights will have (so it regulates how fast the learning process is)\n",
    "# - if \"lr\" is too small your model can stuck and never reach the optimal model\n",
    "# - if \"lr\" is too large your model can overshoot the optimal model\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# set model to training mode\n",
    "model.train()\n",
    "\n",
    "# training loop\n",
    "for epoch in range(nepochs):\n",
    "\n",
    "    # the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 1. forward pass\n",
    "    y_hat = model(X_train)\n",
    "\n",
    "    # 2. compute the loss\n",
    "    loss = loss_function(y_hat, y_train)\n",
    "\n",
    "    # 3. backward pass and optimize weights\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # show how big the loss is\n",
    "    print(f'Epoch {epoch}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you comment on the line which freezes the random number generator and click on run button several times, you will see that the loss values are different every time you re-initialize and train your model. It is because the initial weights are set to random numbers, so the training outcome is not pre-determine.\n",
    "\n",
    "This is why for the learning purposes we need to freeze the state. Please remove the comment and make sure the results are reproducible.\n",
    "\n",
    "Let's see how it performs on the training set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model to predictions mode\n",
    "model.eval()\n",
    "\n",
    "# apply model to training set\n",
    "y_hat = model.forward(X_train)\n",
    "\n",
    "# convert output to numpy array\n",
    "y_hat_arr = y_hat.t().detach().numpy()\n",
    "\n",
    "# combine with reference values\n",
    "res = pd.DataFrame({\n",
    "    \"Reference\": d_train[\"Species\"],\n",
    "    \"Predicted\": y_hat_arr[0]\n",
    "})\n",
    "\n",
    "# compute statistics\n",
    "TP = sum((res[\"Reference\"] == \"virginica\") & (res[\"Predicted\"] > 0))\n",
    "TN = sum((res[\"Reference\"] != \"virginica\") & (res[\"Predicted\"] < 0))\n",
    "FP = sum((res[\"Reference\"] != \"virginica\") & (res[\"Predicted\"] > 0))\n",
    "FN = sum((res[\"Reference\"] == \"virginica\") & (res[\"Predicted\"] < 0))\n",
    "\n",
    "sens = TP / (TP + FN)\n",
    "spec = TN / (TN + FP)\n",
    "acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "(sens, spec, acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for self training on 20 epochs. And since this is a very simple one neuron model you can get and look at the weights and bias for this neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show weights and bias of the trained model\n",
    "(model.layer1.weight, model.layer1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the most important input is in this case is *Petal Length*, it has the biggest weight of $0.56$. The least important is the second one (*Sepal Width*) it has a weight of $-0.03$. The first input (*Sepal Length*) contribute negatively in this combination as well as the last one. \n",
    "\n",
    "In other words, our ANN implements the following linear model:\n",
    "\n",
    "$\\hat{Y} = -0.36 \\times X_1 - 0.03 \\times X_2 + 0.56 \\times X_3 - 0.27 \\times X_4 + 0.07$\n",
    "\n",
    "\n",
    "Now we can apply our model to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare X and y values for the training set\n",
    "X_test = torch.tensor(d_test.iloc[:, 1:5].values).float()\n",
    "\n",
    "c_test = d_test[\"Species\"]\n",
    "y_test = (c_test == \"virginica\") * 2.0 - 1.0\n",
    "y_test = torch.tensor([y_test.values]).float().t()\n",
    "\n",
    "# apply model to test set\n",
    "y_hat = model.forward(X_test)\n",
    "\n",
    "# convert output to numpy array\n",
    "y_hat_arr = y_hat.t().detach().numpy()\n",
    "\n",
    "# combine with reference values\n",
    "res = pd.DataFrame({\n",
    "    \"Reference\": c_test,\n",
    "    \"Predicted\": y_hat_arr[0]\n",
    "})\n",
    "\n",
    "# compute statistics\n",
    "TP = sum((res[\"Reference\"] == \"virginica\") & (res[\"Predicted\"] > 0))\n",
    "TN = sum((res[\"Reference\"] != \"virginica\") & (res[\"Predicted\"] < 0))\n",
    "FP = sum((res[\"Reference\"] != \"virginica\") & (res[\"Predicted\"] > 0))\n",
    "FN = sum((res[\"Reference\"] == \"virginica\") & (res[\"Predicted\"] < 0))\n",
    "\n",
    "sens = TP / (TP + FN)\n",
    "spec = TN / (TN + FP)\n",
    "acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "(sens, spec, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test set it works even better (perhaps because the test set is smaller).\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Now play a bit with this code. Try to remove `torch.manual_seed()` instruction and run training several times. Try to increase the number of epochs and see how it influences the quality of classification both for training set and test set. Try to change the learning rate (make it smaller or larger). Get the best possible model and report the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function and neuron layers \n",
    "\n",
    "How to make the model even more efficient? Well, the most obvious way is to increase the number of layers and connect them all together. However, the total number of weights in this case will multiply, and you can end up with a model with thousands of weights to tune. In this case, you need more objects in order to train, validate, and test it properly.\n",
    "\n",
    "The second way to make an ANN model more efficient is to make it non-linear. The simplest way to introduce non-linearity is to supplement every linear neuron with what is called an *activation function*. \n",
    "\n",
    "The activation function changes the output depending on its value. The simple activation function is called **Rectified Linear Unit (ReLU)**. It works as follows: if the output is negative, it makes it equal to zero. But when output is positive, it simply keeps it as is.\n",
    "\n",
    "On the image below, you can see two different activation functions. The blue one is ReLU:\n",
    "\n",
    "<img src=\"./illustrations/ReLU_and_GELU.svg\" style=\"width:300px\">\n",
    "\n",
    "\n",
    "Let's implement ANN with 3 layers. The first layer will consist of 8 neurons. Every neuron will have 4 inputs and 1 output, so this layer will have 8 outputs. The second layer will consist of 4 neurons, each neuron has 8 inputs and 1 output, so this layer will have four outputs. Finally, the last layer will be the same as we have in our model now — one neuron with 4 inputs and 1 output. \n",
    "\n",
    ">**Note to teacher**<br>draw the architecture on a black board.\n",
    "\n",
    "Neurons in the first two layers will also have ReLU activation function for the output.\n",
    "\n",
    "Here is the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class NewModel(nn.Module):\n",
    "    \"\"\" class for three layers ANN \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NewModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(4, 8)  # first layer: 4 inputs and 8 outputs\n",
    "        self.layer2 = nn.Linear(8, 4)  # second layer: 8 inputs and 4 outputs\n",
    "        self.layer3 = nn.Linear(4, 1)  # third layer: 4 inputs and 1 output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x)) # pass inputs through the first layer and apply ReLU activation\n",
    "        x = F.relu(self.layer2(x)) # pass output from layer 1 through the second layer + ReLU activation\n",
    "        y_hat = self.layer3(x) # pass output from layer 2 through the third layer (no relu)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the code which implements the rest (training and testing). \n",
    "\n",
    "As you can see in this case, for each epoch, we compute the loss for the training set and the test set. So we can detect situations when the model gets worse for the test set and stop. This process is called *validation* and it lets us avoid overtraining the model when it works perfectly for the training set and badly for the test set. This situation is also called *overfitting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use another manual seed here to get reproducible outcome\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# number of epochs to train the model\n",
    "nepochs = 300\n",
    "\n",
    "# define a loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# initialize the new model\n",
    "model = NewModel()\n",
    "\n",
    "# define optimizer which will compute gradients — do the learning.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(nepochs):  # Number of training epochs\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_hat_train = model(X_train)\n",
    "    train_loss = loss_function(y_hat_train, y_train)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # validate\n",
    "    model.eval()\n",
    "    y_hat_test = model(X_test)\n",
    "    test_loss = loss_function(y_hat_test, y_test)\n",
    "\n",
    "    # show how big the loss is\n",
    "    print(f'Epoch {epoch}, train loss: {train_loss.item():.4f} - test loss {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we use more epochs in this case as the model is a bit more complex. \n",
    "\n",
    "Let's see how it performs on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model to prediction mode\n",
    "model.eval()\n",
    "\n",
    "# apply model to test set\n",
    "y_hat = model.forward(X_test)\n",
    "\n",
    "# convert output to numpy array\n",
    "y_hat_arr = y_hat.t().detach().numpy()\n",
    "\n",
    "# combine with reference values\n",
    "res = pd.DataFrame({\n",
    "    \"Reference\": c_test,\n",
    "    \"Predicted\": y_hat_arr[0]\n",
    "})\n",
    "\n",
    "# compute statistics\n",
    "TP = sum((res[\"Reference\"] == \"virginica\") & (res[\"Predicted\"] > 0))\n",
    "TN = sum((res[\"Reference\"] != \"virginica\") & (res[\"Predicted\"] < 0))\n",
    "FP = sum((res[\"Reference\"] != \"virginica\") & (res[\"Predicted\"] > 0))\n",
    "FN = sum((res[\"Reference\"] == \"virginica\") & (res[\"Predicted\"] < 0))\n",
    "\n",
    "sens = TP / (TP + FN)\n",
    "spec = TN / (TN + FP)\n",
    "acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "(sens, spec, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned above, strictly speaking, for this last step, we have to use another set of samples, independent of what was used for training and validation. We break this rule here for illustration purposes only because our data set is small.\n",
    "\n",
    "We can visualize the predicted values in order to get better understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(res[\"Reference\"], res[\"Predicted\"])\n",
    "plt.plot(plt.xlim(), [0, 0], color = \"black\", linestyle=\"--\")\n",
    "plt.ylabel(\"Predicted output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It must be noted that neither the network we use in the last example nor the selected loss function are specifically good for classification. But even with this selection, the result looks good. Let's learn how to improve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classification \n",
    "\n",
    "One of the most common ways to make a classification model with ANN is to use the output layer with several neurons — one for each class. So, for binary classification, we will get two outputs, for classification among three classes — three, and so on. But how to make the classification decision in this case and which loss function to use?\n",
    "\n",
    "The idea is similar to voting. The decision is made by selecting the output with the largest value. For example, if the predicted values in the output layer are [0.23, 0.89, -0.01], then the second output \"wins\", so the predicted label index will be 1 (remember, in Python, indices start from 0, so we have 0, 1, and 2 instead of 1, 2, and 3). Which means the following:\n",
    "\n",
    "1. We need to have as many outputs in the last (output) layer as we have classes.\n",
    "2. We need to create a vector with reference class indices.\n",
    "3. We need to use a special loss function which works best in this case.\n",
    "\n",
    "Let's implement this by creating a model which will predict the class label for the Iris data. This time, any of the three labels. Let's assume that label `\"setosa\"` will have index 0, `\"versicolor\"` will have index 1, and `\"virginica\"` will have index 2. \n",
    "\n",
    "Let's create a dictionary with the indices and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector with classes, so we can get a label by its index\n",
    "classes = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "\n",
    "# create dictionary so we can get index by label\n",
    "class_to_idx = {\"setosa\": 0, \"versicolor\": 1, \"virginica\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's prepare the datasets. We already did this above, but let's do repeat this again and this time let's also create vector with reference class indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensor with predictor values\n",
    "X_values = d.iloc[:, 1:5].values\n",
    "X = torch.tensor(X_values).float()\n",
    "\n",
    "# create tensor with class indices\n",
    "label_values = [class_to_idx[label] for label in d[\"Species\"]]\n",
    "labels = torch.tensor(label_values).long()\n",
    "\n",
    "# show it on the screen to check\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you remember, adding `float()` to the tensor makes the values floating point numbers. Adding `long()` makes them integer values. Having label indices as integer numbers is required by the loss function we are going to use.\n",
    "\n",
    "Now let's split both tensors to training and test set like we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate indices of rows for training and test set\n",
    "train_ind = d[\"Id\"] % 5 != 0\n",
    "test_ind = d[\"Id\"] % 5 == 0\n",
    "\n",
    "# select rows and label indices for the training set\n",
    "X_train = X[train_ind, :]\n",
    "labels_train = labels[train_ind]\n",
    "\n",
    "# select rows and label indices for the test set\n",
    "X_test = X[test_ind, :]\n",
    "labels_test = labels[test_ind]\n",
    "\n",
    "# show test set labels\n",
    "labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this time instead of making subsets for the data frame, we created tensors first and then subset the tensors. This way is a bit shorter and perhaps more clear.\n",
    "\n",
    "Now let's create a new class with ANN model with three outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiClassModel(nn.Module):\n",
    "    \"\"\" class for three layers ANN \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MultiClassModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(4, 8)  # first layer: 4 inputs and 8 outputs\n",
    "        self.layer2 = nn.Linear(8, 16)  # first layer: 8 inputs and 16 outputs\n",
    "        self.layer3 = nn.Linear(16, 3)  # third layer: 16 inputs and 3 outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x)) # pass inputs through the first layer and apply ReLU activation\n",
    "        x = F.relu(self.layer2(x)) # pass output from layer 1 through the second layer + ReLU activation\n",
    "        y_hat = self.layer3(x) # pass output from layer 2 through the third layer (no relu)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model is very similar to what we had before, we just changed the number of inputs/outputs. \n",
    "\n",
    ">**Note to teacher:**<br>draw the model schematically on a black board.\n",
    "\n",
    "Now, we need to define the loss function. In this particular case, when the decision is made by voting, the best suitable function is cross-entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a loss function\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is ready, let's initialize and train the model (as in the examples above, we will freeze the random number generator state to get reproducible results):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed the random number generator to get reproducible outcome\n",
    "torch.manual_seed(12)\n",
    "\n",
    "# initialize the new model\n",
    "model = MultiClassModel()\n",
    "\n",
    "# define optimizer which will compute gradients — do the learning.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# number of epochs to train the model\n",
    "nepochs = 100\n",
    "\n",
    "for epoch in range(nepochs):  # Number of training epochs\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    labels_predicted = model(X_train)\n",
    "\n",
    "    loss = loss_function(labels_predicted, labels_train)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    # validate\n",
    "    model.eval()\n",
    "    labels_predicted = model(X_test)\n",
    "    loss = loss_function(labels_predicted, labels_test)\n",
    "    val_loss = loss.item()\n",
    "\n",
    "    # show how big the loss is\n",
    "    print(f'Epoch {epoch}, train loss: {train_loss:.4f} - validation loss {val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good, although, as you can see, the validation loss continues to decrease, so perhaps 100 epochs is not enough. We will come back to this later. Let's learn how to make predictions.\n",
    "\n",
    "Let's make predictions for both sets and look at the predicted values for the test set first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model to prediction mode\n",
    "model.eval()\n",
    "\n",
    "# apply model to train and test sets\n",
    "output_train = model.forward(X_train)\n",
    "output_test = model.forward(X_test)\n",
    "\n",
    "# show predictions for the test set\n",
    "output_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look carefully, you can find out that indeed, for the first ten rows (where we have *setosa* samples), the first out of the three values is the largest. For the other two, it is not so clear. Let's apply `max` function to each row and ask not the value but the position (index) where the largest value is located:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of largest values for each row of computed outputs\n",
    "_, predictions_train = torch.max(output_train, 1)\n",
    "_, predictions_test = torch.max(output_test, 1)\n",
    "\n",
    "predictions_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the predicted class label index for setosa is good, but the others are not perfect so far. We will fix it later. \n",
    "\n",
    "Because here we have three classes, it will be to laborious to compute classification statistics for each. Let's learn a new way to assess classification results — via contingency table or [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). It simply shows all possible combinations of the reference and the predicted class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# compute contingency table for training and test sets\n",
    "ct_train = np.zeros((3, 3))\n",
    "ct_test = np.zeros((3, 3))\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ct_train[i, j] = sum((labels_train == i) & (predictions_train == j))\n",
    "        ct_test[i, j] = sum((labels_test == i) & (predictions_test == j))\n",
    "\n",
    "(ct_train, ct_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table for ideal classification should have zero for all values except the diagonal (where the predicted and reference label index match). As you can see, we have perfect matches for the first (0, \"setosa\") and the second (2, \"versicolor\") classes, but the \"virginica\" class is not well predicted. Six flowers of this class were wrongly predicted as *versicolor* and four were correctly predicted as *virginica*.\n",
    "\n",
    "You can also compute this table for relative values (percent), which is easier to use when the number of individuals in different classes is not the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute contingency table for training and test set with relative values\n",
    "ct_train = np.zeros((3, 3))\n",
    "ct_test = np.zeros((3, 3))\n",
    "for i in range(3):\n",
    "    n_train = sum(labels_train == i)\n",
    "    n_test = sum(labels_test == i)\n",
    "    for j in range(3):\n",
    "        ct_train[i, j] = sum((labels_train == i) & (predictions_train == j)) / n_train\n",
    "        ct_test[i, j] = sum((labels_test == i) & (predictions_test == j)) / n_test\n",
    "\n",
    "(ct_train, ct_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's learn how to visualize this by making a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap plot for contrast table\n",
    "plt.imshow(ct_test, clim = [0, 1])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add class labels and show the numbers for each cell of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap plot for contrast table\n",
    "plt.imshow(ct_test, clim = [0, 1])\n",
    "plt.colorbar()\n",
    "plt.gca().set_xticks(range(3), classes)\n",
    "plt.gca().set_yticks(range(3), classes)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        plt.text(i, j, round(ct_test[j, i], 3), color = \"white\" if ct_test[j, i] < 0.5 else \"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create several functions in order to re-use the code we have written above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, labels_train, X_val, labels_val, nepochs = 100, lr = 0.01):\n",
    "    \"\"\" trains any classification model using provided data, number of epochs and learning rate \"\"\"\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        labels_predicted = model(X_train)\n",
    "        loss = loss_function(labels_predicted, labels_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        labels_predicted = model(X_val)\n",
    "        loss = loss_function(labels_predicted, labels_val)\n",
    "        val_loss = loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch}, train loss: {train_loss:.4f} - validation loss {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X):\n",
    "    \"\"\" get ANN model and tensor with predictors and returns predicted class label indices \"\"\"\n",
    "    model.eval()\n",
    "    output = model.forward(X)\n",
    "    _, labels = torch.max(output, 1)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table(reference, predicted):\n",
    "    \"\"\" computes contingency table for predicted and reference class label indices \"\"\"\n",
    "    indices = reference.unique()\n",
    "    n = len(indices)\n",
    "    ct = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        ni = sum(reference == indices[i])\n",
    "        for j in range(n):\n",
    "            ct[i, j] = sum((reference == indices[i]) & (predicted == indices[j])) / ni\n",
    "\n",
    "    return ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ct_heatmap(ct, classes):\n",
    "    \"\"\" shows heatmap for the contingency table \"\"\"\n",
    "    plt.imshow(ct, clim = [0, 1])\n",
    "    plt.colorbar()\n",
    "\n",
    "    n = len(classes)\n",
    "    plt.gca().set_xticks(range(n), classes)\n",
    "    plt.gca().set_yticks(range(n), classes)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            plt.text(i, j, round(ct[j, i], 3), color = \"white\" if ct[j, i] < 0.5 else \"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the function `train()` does not return anything. It is because `model`, which we pass to this function as a first argument, is an object, it has its own methods that update this object internally. So although all training happens inside this function, the object outside gets all the necessary updates.\n",
    "\n",
    "Moreover, we can use this function for any other models as well.\n",
    "\n",
    "Let's check how the new functions work. Let's train and re-initialite the model and train it again using more epochs and smaller learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12)\n",
    "model = MultiClassModel()\n",
    "train(model, X_train, labels_train, X_test, labels_test, nepochs = 1000, lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = predict(model, X_train)\n",
    "predictions_test = predict(model, X_test)\n",
    "\n",
    "ct_train = table(labels_train, predictions_train)\n",
    "ct_test = table(labels_test, predictions_test)\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "ct_heatmap(ct_train, classes)\n",
    "plt.title(\"Train\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "ct_heatmap(ct_test, classes)\n",
    "plt.title(\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 1000 epochs, smaller learning rate, the new architecture works almost perfectly!\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "In file `IrisHeatmap.csv` you will find new data points which contain new measurements but do not have column with species nor column with IDs (so it has only four columns with measurements). Load the data from the file and then apply the ANN you have just trained to get the predictions.\n",
    "\n",
    "After that make a scatter plot, where x- and y-values are Petal Length and Petal Width you got from the file. Show points, predicted as *setosa* using red color points, predicted as *versicolor* as green and points, predicted as *virginica* as blue. Comment the plot. What it actually shows? \n",
    "\n",
    "Try to add predictions for training and test set to the plot. In this case you will need to make the predictions for new data shown semi-transparent. Use parameter `alpha = 0.15` in the `plt.scatter()` function for that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the training process\n",
    "\n",
    "It can be useful to see how training and validation loss changes with epochs. Let's modify the `train_model()` method in order to collect this information and return it back to user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, labels_train, X_test, labels_test, nepochs = 100, lr = 0.01):\n",
    "    \"\"\" trains any classification model using provided data, number of epochs and learning rate \"\"\"\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    train_losses = np.zeros(nepochs)\n",
    "    val_losses = np.zeros(nepochs)\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        labels_predicted = model(X_train)\n",
    "        loss = loss_function(labels_predicted, labels_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses[epoch] = loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        labels_predicted = model(X_test)\n",
    "        loss = loss_function(labels_predicted, labels_test)\n",
    "        val_losses[epoch] = loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch}, train loss: {train_losses[epoch]:.4f} - validation loss {val_losses[epoch]:.4f}')\n",
    "\n",
    "    return (train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model again, this time using 5000 epochs, get the loss values, and make a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed the random number generator to get reproducible outcome\n",
    "torch.manual_seed(12)\n",
    "\n",
    "# re-initialize and train the model\n",
    "model = MultiClassModel()\n",
    "train_losses, val_losses = train(model, X_train, labels_train, X_test, labels_test,\n",
    "                                 nepochs = 5000, lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show plot with both losses\n",
    "plt.plot(train_losses, label = \"train\")\n",
    "plt.plot(val_losses, label = \"val\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim([0, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, after approximately 3000 epochs the validation loss almost does not change. So there is no reason to run the model that far.\n",
    "\n",
    "Let's re-initialize the model (this is needed to set all initial weights to random numbers; otherwise, weights for the already trained model will be used as a starting point) and train it again, but this time we will use a large learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12)\n",
    "model = MultiClassModel()\n",
    "train_losses, val_losses = train(model, X_train, labels_train, X_test, labels_test,\n",
    "                                 nepochs = 5000, lr = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label = \"train\")\n",
    "plt.plot(val_losses, label = \"val\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see some strange patterns, \"peaks\", on the plot. These patterns are typical for the large learning rate when the optimizer changes the weights of the neurons too much, so the model \"jumps\" back and forces. \n",
    "\n",
    "This can be compared with driving a car. If you accelerate and brake slowly, monitor the situation on the road, and take actions in advance (pro-actively), your car will move smoothly and comfortably for you and your passengers. But the situation we see on the plot above corresponds to the case when you accelerate and break every minute.\n",
    "\n",
    "This plot helps to find the optimal learning rate and number of epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load the model state\n",
    "\n",
    "Finally, let's learn how to save and load the model state — all internal parameters, which include weights and biases of all layers, etc. This can be handy in the following situations:\n",
    "\n",
    "1. You can save the model state into a separate variable after every epoch, but only if validation loss gets better. This helps to avoid overfitting when you train your model too long and it performs worse on the validation set than, e.g., 50 epochs before. We will consider this case in the next class.\n",
    "\n",
    "2. Save the model to a file for later use. For example, you can send it to your classmates or colleagues. Or simply save it in order to continue the training process tomorrow. Or to re-use it for predictions. Many possibilities.\n",
    "\n",
    "Let's see first how to get the state of the model. Try to run the next code first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in model.parameters():\n",
    "    print(parameter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a collection of torch tensors (similar to NumPy arrays as we remember). For example, first tensor has 8 rows and 4 columns. Can you guess why?\n",
    "\n",
    "Because in the first layer of your model, you have 8 neurons. Every neuron has 4 inputs and 1 output. And output is computed by taking the weighted sum of the inputs plus bias. So for every neuron, you need 5 parameters — 4 weights and 1 bias. \n",
    "\n",
    "The first tensor of size 8x4 contains the weight for every of the 8 neurons, and the second tensor (a vector with 8 values) contains the biases.\n",
    "\n",
    "The second layer has 16 neurons, every neuron has 8 inputs and 1 output. So every neuron has 9 parameters — 8 weights and 1 bias. This is exactly what you have in the next pair of tensors, the one of size 16x8 contains weights, and the one with 16 values contains the biases.\n",
    "\n",
    "Finally, the last layer has 3 neurons with 16 inputs and 1 output each, so the last two tensors contain weights and biases for these neurons.\n",
    "\n",
    "In fact, you can see the structure of your model and the number of parameters by using method `summary()` from the library `torchinfo`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also compute the total number of parameters of your model manually by simply taking a sum of all elements in each tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npar = 0\n",
    "for parameter in model.parameters():\n",
    "    npar = npar + parameter.numel()\n",
    "\n",
    "print(f\"Total number of parameters in this ANN: {npar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can of course see all the weights (the output will be long though):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in model.parameters():\n",
    "    print(\"------\")\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strictly speaking, you can take all these values, copy-paste them to Excel, and run predictions in Excel (you just need to remember to apply ReLU function to each output). Crazy idea, but it is doable. This shows that ANN is not rocket science, but very simple, straightforward, yet powerful method.\n",
    "\n",
    "There is another way to get the model parameters with all the necessary additional information — state dictionary. There is a method for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the output is similar to what we have seen before, but this time it is organized as a dictionary, so Torch will know a level name and the parameter name.\n",
    "\n",
    "We can save the state to a variable by taken a deep copy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to load a special function which creates copy of complex objects\n",
    "from copy import deepcopy\n",
    "\n",
    "# save parameters of current model to a variable\n",
    "model_state = deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you need to take a deep copy instead of just assigning the state to a new variable? Because if you continue training your model, this state will also get all updates. By taking a deep copy, you kind of disjoint the current state from the next ones, making it independent.\n",
    "\n",
    "Here is an example how you can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed the random number generator to get reproducible outcome\n",
    "torch.manual_seed(12)\n",
    "\n",
    "# initialize a new model\n",
    "new_model = MultiClassModel()\n",
    "\n",
    "# make predictions — they will be very bad because the model is not trained\n",
    "predictions = predict(new_model, X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets load the parameters we saved from the trained model to this new model\n",
    "new_model.load_state_dict(model_state)\n",
    "\n",
    "# and make predictions\n",
    "predictions = predict(new_model, X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we got perfect predictions without training the new model, but just by reusing the parameters of the previously trained model.\n",
    "\n",
    "If you want to save the state to a file and send it to someone or load it later in another Python script, you can use two PyTorch functions: `torch.save()` saves the model dictionary to a file, and `torch.load()` loads it from the file. The file should have the extension `.pth`. \n",
    "\n",
    "Let's see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save state dictionary to file\n",
    "torch.save(model.state_dict(), \"mymodel.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we do not need to take a deepcopy in this case because it will be in a separate file. If you run it, you will get the file, `mymodel.pth` inside the current folder.\n",
    "\n",
    "There is no reason to open it, as inside the information inside is coded in binary format. But you can load it and assign it to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new model with random weights\n",
    "another_model = MultiClassModel()\n",
    "\n",
    "# load state from the file and assign it to the new model\n",
    "model_state = torch.load(\"mymodel.pth\")\n",
    "another_model.load_state_dict(model_state)\n",
    "\n",
    "# make predictions\n",
    "predictions = predict(another_model, X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!\n",
    "\n",
    "### Exercise \n",
    "\n",
    "To do this exercise, you need to work in pairs. One of the members should train the ANN network on Iris data (make a separate notebook and copy all the needed code there). Train it using random initialization and try different hyperparameters (number of epochs, learning rate, etc.). When you get a good model, save it to a file and send it to your groupmate by email. The task of the groupmate is to load the model and apply it to the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Play with interactive ANN in web\n",
    "\n",
    "Now you know everything you need to know for the next class. However, we recommend you play a bit more using this interactive ANN constructor, which you can run directly in your web-browser. Spend some time and play with different problems, different architectures, and hyperparameters:\n",
    "\n",
    "[TensorFlow playground](http://playground.tensorflow.org)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
