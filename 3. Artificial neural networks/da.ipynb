{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Kunstige neurale netværk\n",
    "\n",
    "Kunstige neurale netværk (ANNs) giver en alsidig måde at opbygge enhver maskinlæringsmodel på. Ideen er inspireret af strukturen og funktionen af den menneskelige hjerne. Neurale netværk har vist sig at være kraftfulde til at løse komplekse problemer med klassifikation og andre opgaver. I denne klasse vil vi forklare grundlæggende om neurale netværk, deres komponenter, træningsprocessen og validering af ANNs.\n",
    "\n",
    "Der findes flere biblioteker til Python, der implementerer ANN. De mest kendte er [Tensorflow](https://www.tensorflow.org) og [PyTorch](https://pytorch.org) (der findes også et bibliotek, som tager det bedste fra begge, [Keras](https://keras.io)). Til eksemplerne i denne klasse vil vi bruge PyTorch, som i disse dage sandsynligvis er det mest udbredte og vel dokumenterede.\n",
    "\n",
    "Installer biblioteket ved at køre følgende kode (efter det, tilføj `#` foran denne linje for at undgå at køre den igen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som du måske har lagt mærke til, har vi installeret to biblioteker her, PyTorch (`torch`) og et supplement, der vil hjælpe os med at opdage strukturen af ANNs (`torchinfo`).\n",
    "\n",
    "Inden vi fortsætter, lad os splitte vores data op i henholdsvis træningssæt og testsæt igen, som vi gjorde det i den forrige klasse. Vi vil senere bruge træningssættet til læreprocessen og testsættet til at vurdere, hvor god den trænede model er.\n",
    "\n",
    "Strengt taget har vi i dette tilfælde brug for tre sæt:\n",
    "\n",
    "- *træningssæt*, som bruges til læring\n",
    "- *valideringssæt*, som bruges til at optimere modellen (finde den bedste under læring)\n",
    "- *testsæt*, som bruges til at vurdere kvaliteten af den endelige model\n",
    "\n",
    "For at forenkle eksemplerne vil vi bruge *testsættet* til både validering og test, men når du arbejder med reelle tilfælde, så sørg for at de er adskilte, og du har alle tre sæt (vi vil bruge denne tilgang i den næste klasse).\n",
    "\n",
    "Som i den forrige klasse tager vi simpelthen hver femte måling som testsæt, og senere vil vi lære, hvordan man gør det på en bedre måde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from CSV file as data frame\n",
    "import pandas as pd\n",
    "d = pd.read_csv(\"Iris.csv\")\n",
    "\n",
    "# generate logical values for train and test set measurements (rows of data frame)\n",
    "train_ind = d[\"Id\"] % 5 != 0\n",
    "test_ind = d[\"Id\"] % 5 == 0\n",
    "\n",
    "# make the split\n",
    "d_train = d.loc[train_ind]\n",
    "d_test = d.loc[test_ind]\n",
    "\n",
    "# show size of each set\n",
    "(d_train.shape, d_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvad er et neuralt netværk?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En neural netværk er simpelthen et sæt noder, kendt som *neuroner*, der er forbundet med hinanden. I den simpleste tilfælde består et neuralt netværk kun af en node som vist på billedet nedenfor.\n",
    "\n",
    "<img src=\"./illustrations/Neuron.png\" style=\"width:500px; height:400px;\"/>\n",
    "\n",
    "Hver neuron har en række inputs (vist som $X_1$, $X_2$, $X_3$ og $X_4$ på venstre side af billedet) og et output (vist som $\\hat{Y}$ på højre side). Den udfører en meget simpel opgave - den tager alle tal, som den modtager fra inputtet, og anvender en matematisk funktion, som beregner en outputværdi baseret på inputtet.\n",
    "\n",
    "I den simpleste tilfælde beregner den en vægtet sum (kendt som *linearkombination*) af disse tal og sender det beregnede tal til outputtet. Matematisk kan vi skrive dette som følger:\n",
    "\n",
    "$\\hat{Y} = (X_{1}\\times W_{1}) + (X_{2}\\times W_{2}) + (X_{3}\\times W_{3}) + (X_{4}\\times W_{4}) + Bias$\n",
    "\n",
    "Værdierne $W_1$,...,$W_4$ er *vægte*, som hvert input bidrager til outputtet med.\n",
    "\n",
    "Forestil dig, at inputtene er målingerne fra den første række af Iris-datasættet: \n",
    "\n",
    "$X = [5.1,3.5,1.4,0.2]$ \n",
    "\n",
    "og vægtene er: \n",
    "\n",
    "$W = [0.1, 0.2, 2.0, 5.0]$. \n",
    "\n",
    "Lad os antage, at bias er $1.0$. Så vil outputværdien for vores neuron være:\n",
    "\n",
    "$\\hat{Y} = 5.1 \\times 0.1 + 3.5 \\times 0.2 + 1.4 \\times 2.0 + 0.2 \\times 5.0 + 1.0 = 6.01$\n",
    "\n",
    "Så simpelt er det.\n",
    "\n",
    "Her er hvordan vi kan implementere dette ene neuron-baserede kunstige neurale netværk i Python ved hjælp af PyTorch biblioteket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    \"\"\" class for one-neuron ANN model \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # initialize the parent (super) class for ANN model\n",
    "        super(SimpleModel, self).__init__()\n",
    "        # define all layers with neurons and their properties\n",
    "        self.layer1 = nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" takes vector with input values 'x', computes and returns the output \"\"\"\n",
    "        y_hat = self.layer1(x)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som du kan se, koden er lidt mere kompleks sammenlignet med det, vi har haft før. I denne kode opretter vi en *klasse* `SimpleModel` oven på en anden klasse `nn.Module`, som allerede er implementeret i PyTorch.\n",
    "\n",
    ">Du kan tænke på en klasse som en opskrift, en byggevejledning, et sæt instruktioner. Forestil dig, at du vil bygge et hus. Der er et sæt grundlæggende instruktioner, der fortæller, hvordan man bygger fundamentet til huset, lægger afløbsrør osv. Derudover har det nogle typiske instruktioner, f.eks. hvordan man bygger en mur af mursten. Der er ingen grund til at genopfinde den dybe tallerken, så når du vil lave instruktioner til at bygge et helt hus (et typisk projekt), med vægge, tag osv., kan du tage det grundlæggende sæt som udgangspunkt og udvide det med din egen del.\n",
    "\n",
    "Samme idé her, `nn.Model` er et grundlæggende sæt instruktioner om, hvordan man laver et neuralt netværk i PyTorch, og det inkluderer en masse ting, du ikke behøver at bekymre dig om. Du tager det simpelthen som basis og udvider det med dine egne specifikke dele - hvilke noder du vil bruge, hvor mange input de har, hvor mange output osv. Så denne nye instruktion er klassen `SimpleModel`.\n",
    "\n",
    "Som du kan se, har vi tilføjet to metoder til klassen.\n",
    "\n",
    "Den første metode, `__init__`, er nødvendig for at initialisere din model. I begyndelsen indeholder den:\n",
    "\n",
    "```python\n",
    "super(SimpleModel, self).__init__()\n",
    "```\n",
    "\n",
    "hvilket fortæller Python at foretage alle forberedelser baseret på det grundlæggende sæt instruktioner (f.eks. lave husets fundament).\n",
    "\n",
    "Og derefter definerer du dit netværk - hvor mange neuroner, hvilken type neuroner og definerer deres egenskaber. I dette tilfælde har vi en lineær neuron med 4 input og 1 ouput - præcis hvad vi brugte i eksemplet ovenfor.\n",
    "\n",
    "**Du kan tænke på `__init__()` som en metode, der bygger huset ved hjælp af dine instruktioner.** Ikke malet, uden møbler, men et helt, fuldt fungerende hus.\n",
    "\n",
    "Den anden metode i denne klasse, `forward()`, bruges hver gang du vil anvende din model på givne input for at producere outputtet. Metoden \"forbinder\" neuronerne, den sikrer, at input går gennem neuronerne i korrekt rækkefølge og returnerer det resulterende output til sidst.\n",
    "\n",
    "**Du kan tænke på `forward()` som en metode, der giver dig mulighed for at bruge det hus, du har skabt.** Den bruges altid i dens nuværende tilstand. Så hvis du anvender metoden `forward()` på et nyoprettet hus, vil det ikke fungere godt, da dit hus har bare vægge og ingen møbler. Men du kan selvfølgelig bo der.\n",
    "\n",
    "Her er et eksempel på, hvordan vi kan bruge modellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = SimpleModel()\n",
    "\n",
    "# define values for input\n",
    "X = torch.tensor([[5.1, 3.5, 1.4, 0.2]])\n",
    "\n",
    "# send input to the model and get the output\n",
    "y_hat = model(X)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man kan se, at hver gang vi skal give nogle tal til PyTorch, er det ikke nok bare at kombinere dem til en 1D-liste ved at bruge firkantede parenteser, f.eks.:\n",
    "\n",
    "```python\n",
    "X = [5.1, 3.5, 1.4, 0.2] \n",
    "```\n",
    "\n",
    "eller lave en 2D-liste som:\n",
    "\n",
    "```python\n",
    "X = [[5.1, 3.5, 1.4, 0.2]]\n",
    "```\n",
    "\n",
    "Vi skal også konvertere listen til en speciel type, `torch.tensor`. Dette ligner det, vi gjorde i den første klasse, for at oprette NumPy-arrays, vi gav værdier som en liste og brugte derefter en speciel metode, der fortæller Python - lav det til et NumPy array:\n",
    "\n",
    "```python\n",
    "X = np.array([[5.1, 3.5, 1.4, 0.2]])\n",
    "```\n",
    "\n",
    "Visuelt set er *Tensor* det samme som et array, det kan være 1D (vektor), 2D (matrix), 3D osv. Så Torch tensor ligner NumPy array, som du allerede kender. Men i virkeligheden er der en forskel. For Python er de to forskellige dataobjekter fra to forskellige biblioteker, og du kan anvende forskellige metoder og operatører på hver af dem. Det er derfor vigtigt at \"fortælle\" Python, at dette ikke bare er en liste eller NumPy array, men en PyTorch tensor.\n",
    "\n",
    ">Et af eksemplerne her kan være biler og servicecentre. Hvis du har en Ford, vil du ikke gå til Hyundais servicecenter for f.eks. at skifte bremser og motorolie. Fordi deres mekanikere ikke er certificeret til at arbejde med dette mærke. Selvom begge er biler, ser de lignende ud, fungerer lignende, og du kan nemt køre begge, er de ikke identiske.  \n",
    "\n",
    "En af grundene til at bruge Torch tensorer i stedet for NumPy arrays er, at tensorerne er designet til at arbejde med [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit) - de kraftfulde grafikkort, som vi f.eks. har i spil computere. Derudover fungerer det kun med specifikke GPU'er, fremstillet af virksomheden [NVIDIA](https://www.nvidia.com/da-dk/geforce/graphics-cards/). Hvis du er gamer, har du sandsynligvis hørt om grafikkort som GTX 3090. Da neurale netværk kræver en masse computerkraft, gør brugen af GPU dem meget hurtigere, og PyTorch er designet til at arbejde på en GPU først. Hvis du ikke har en GPU, kan den også arbejde på en konventionel processor, men beregningen vil være langsom. Især hvis du har store datasæt med tusindvis af objekter.\n",
    "\n",
    "Så selvom det er lidt irriterende, at vi skal tilføje `torch.tensor()`, vil du senere finde ud af, at det ikke er et stort problem.\n",
    "\n",
    "Hvis vi vender tilbage til vores kode og dens beregnede værdi, kan du se, at outputtet ikke er $6.01$, desuden hver gang du kører denne kode (prøv at klikke på *Kør*-knappen flere gange), vil du få en ny output. Fordi når du initialiserer ANN-modellen, bruger den tilfældige tal for dine vægte.\n",
    "\n",
    "Du kan ændre dette og tildele vægtene manuelt efter initialisering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define weights and bias\n",
    "W = torch.tensor([[0.1, 0.2, 2.0, 5.0]])\n",
    "Bias = torch.tensor([1.0])\n",
    "\n",
    "# set weights and bias manually for the neurons on layer \"layer1\"\n",
    "model.layer1.weight = nn.Parameter(W)\n",
    "model.layer1.bias = nn.Parameter(Bias)\n",
    "\n",
    "# forward pass through the model\n",
    "y_hat = model(X)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu er det $6,01$! Du kan bemærke, at værdien, vi får som output, også er en tensor.\n",
    "\n",
    "Forresten, med Torch kan du også beregne lineære kombinationer manuelt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X * W).sum() + Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lad os nu implementere klassifikationsreglen for *Virginica*-prøverne, som vi oprettede i en tidligere klasse. Hvis du husker, sammenlignede vi Petal bredde ($X_4$ i vores tilfælde) af en blomst med 1,7, og hvis værdien var over denne tærskelværdi, klassificerede vi denne blomst som *Virginica*.\n",
    "\n",
    "Her er de vægte og bias, der kan implementere denne regel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define weights and bias\n",
    "W = torch.tensor([[0.0, 0.0, 0.0, 1.0]])\n",
    "Bias = torch.tensor([-1.7])\n",
    "\n",
    "# set weights and bias manually\n",
    "model.layer1.weight = nn.Parameter(W)\n",
    "model.layer1.bias = nn.Parameter(Bias)\n",
    "\n",
    "# forward pass through the model\n",
    "y_hat = model(X)\n",
    "\n",
    "# show result and apply threshold\n",
    "(y_hat, y_hat > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som du kan se, alle vægte bortset fra den sidste er sat til nul, mens vægten for $X_4$ er lig med én. Dette giver outputtet, $\\hat{Y}$, simpelthen lig med værdien for $X_4$, som i vores tilfælde er Kronbladets bredde. Derefter anvender vi bias, så vi trækker vores tærskel fra. Dette giver følgende:\n",
    "\n",
    "$\\hat{Y} = X_4 - 1.7$\n",
    "\n",
    "Tilsyneladende, hvis *Petal bredde* er under tærsklen, vil outputværdien være negativ, og når den er over, vil værdien være positiv. Så vi kan træffe en klassifikationsbeslutning ved simpelthen at sammenligne den med 0, som vi gjorde ovenfor.\n",
    "\n",
    "Lad os nu anvende dette netværk med manuelt indstillede vægte på testsettet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only columns with measurements and convert them to Torch tensor\n",
    "X_test_values = d_test.iloc[:, 1:5].values\n",
    "X_test = torch.tensor(X_test_values).float()\n",
    "\n",
    "# apply the model\n",
    "y_hat = model(X_test)\n",
    "\n",
    "# show the results\n",
    "y_hat > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi fik faktisk mange negative tal i begyndelsen, hvor vi havde blomster af *Setosa* og *Versicolor* arterne, og positive tal i slutningen, hvor vi havde blomster af målklassen, *Virginica*.\n",
    "\n",
    "Lad os kombinere output og referenceværdierne i et datasæt for bedre synlighed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert output to numpy array (transpose -> detach from model -> convert to NumPy array)\n",
    "y_hat_arr = y_hat.t().detach().numpy()\n",
    "y_hat_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# combine with reference values\n",
    "res = pd.DataFrame({\n",
    "    \"Reference\": d_test[\"Species\"],\n",
    "    \"Predicted\": y_hat_arr[0],\n",
    "    \"Is virginica\": y_hat_arr[0] > 0\n",
    "})\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi fik helt præcis de samme resultater som i den sidste klasse!\n",
    "\n",
    "Men hvordan lader vi modellen finde klassificeringsbeslutningen automatisk, baseret på de givne data? Vi skal træne modellen! Men hvad skal vi bruge som træningskriterie?\n",
    "\n",
    "## Tabsfunktion\n",
    "\n",
    "Hele idéen med træning af et kunstigt neuralt netværk (ANN) er at finde vægtene (og bias og andre parametre, hvis der er nogen) for alle neuroner, som vil gøre outputtet så tæt på det ønskede som muligt.\n",
    "\n",
    "Men hvordan måler vi afstanden mellem modellens output og det ønskede output? Dette er hvad der defineres som en *tabe*. Taben er et tal, en statistik, som fortæller, hvor stor forskellen er mellem det ønskede output, $Y$, og det forudsagte output, $\\hat{Y}$.\n",
    "\n",
    "Som du husker, vil vi lave en klassifikation for *Virginica* blomster. Lad os definere det ideelle output til at være 1 for *virginica* og -1 for de andre. Sådan opretter du det:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the classes labels for the training set\n",
    "c = d_test[\"Species\"]\n",
    "\n",
    "# compare the labels with target class then multiply the result to 2 and subtract 1\n",
    "# if the result is False, it will be treated as 0: 0 * 2 - 1 = -1\n",
    "# if the result is True, it will be treated as 1: 1 * 2 - 1 = 1\n",
    "y_dummy = (c == \"virginica\") * 2 - 1\n",
    "y_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the values from the data frame column to torch array\n",
    "y = torch.tensor([y_dummy.values])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det eneste problem, vi har, er, at værdierne præsenteres som rækker, og vi har brug for dem som en kolonne. Lad os tilføje transponering og også konvertere dem fra heltal til flydende punkt numre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the values to torch array with 1 column and 120 rows\n",
    "y = torch.tensor([y_dummy.values]).float().t()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hver gang vi får output fra modellen, skal vi sammenligne den med disse reference y-værdier og beregne en statistik, der fortæller, hvor stor forskellen er — tabet. Den enkleste statistik, der vil gøre det for dig, kaldes middelkvadreret fejl (MSE). I dette tilfælde tager du simpelthen en forskel mellem de to vektorværdier, kvadrerer denne forskel og beregner gennemsnittet (middelværdien).\n",
    "\n",
    "Her er hvordan man implementerer det:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_hat, y):\n",
    "    \"\"\" computes mean squared error loss \"\"\"\n",
    "    return ((y - y_hat)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu skal vi teste det. Vi har allerede outputtet og de ønskede (reference) værdier for test-sættet, så vi kan beregne tabværdien:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test = mse_loss(y_hat, y)\n",
    "loss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den mindre tabe, jo bedre.\n",
    "\n",
    "I virkeligheden behøver du ikke at beregne tabet manuelt, som vi gjorde i kodeblokken ovenfor. PyTorch har allerede implementeret mange tabfunktioner, der er specielt designet til at blive brugt i træningsprocessen (f.eks. til beregning af gradienter). Her er den for MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "loss_test = loss_function(y_hat, y)\n",
    "\n",
    "loss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som du kan se, giver den værdi, der er identisk med det, vi fik ved brug af vores egen manuelle implementering af MSE-tabet.\n",
    "\n",
    "Som vi nævnte ovenfor, er der mange forskellige måder at måle tabet på, så MSE er ikke den eneste, der bruges. For eksempel bruges i tilfælde af binær klassifikation en anden funktion, *Binary Correlation Entropy* (BCE). I tilfælde af klassifikation med flere klasser kan man bruge *Cross Entropy Loss*.\n",
    "\n",
    "Du behøver ikke at kende dem alle, husk bare at tabfunktionen viser dig (og din model), hvor stor forskellen er mellem den ønskede output og dem, din model beregner nu.\n",
    "\n",
    "Derudover fortæller det modellen, hvordan man beregner gradienter - en række trin, som lader ANN ændre vægtene for at gøre tabet mindre. Denne proces med at reducere tabet ved gradvist at opdatere vægtene kaldes *[gradient descent](https://www.ibm.com/topics/gradient-descent)*, og det er den primære måde at træne enhver ANN-model på. Nu kan vi diskutere træningen i detaljer.\n",
    "\n",
    "### Øvelse 1\n",
    "\n",
    "Udfyld følgende to tabeller (du kan gøre det f.eks. i Excel eller manuelt på papir), beregn MSE for hver, og kommenter på, hvor godt MSE beskriver klassificeringsresultaterne:\n",
    "\n",
    "\n",
    "*Case 1*\n",
    "\n",
    "| $y$ | $\\hat{y}$ | $(y - \\hat{y})$ | $(y - \\hat{y})^2$ |\n",
    "| --:| ---------:| ---------------:| -----------------:|\n",
    "| -1 | 0.2 | - | - |\n",
    "| -1 | 0.4 | - | - |\n",
    "|  1 | -0.2 | - | - |\n",
    "|  1 | -0.4 | - | - |\n",
    "\n",
    "*Case 2*\n",
    "\n",
    "| $y$ | $\\hat{y}$ | $(y - \\hat{y})$ | $(y - \\hat{y})^2$ |\n",
    "| --:| ---------:| ---------------:| -----------------:|\n",
    "|  1 | 0.2 | - | - |\n",
    "|  1 | 0.4 | - | - |\n",
    "| -1 | -0.2 | - | - |\n",
    "| -1 | -0.4 | - | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient og optimering\n",
    "\n",
    "Tabsfunktionen bruges ikke kun til at vurdere kvaliteten af de forudsagte værdier, men også til at beregne gradienter for vægtene—hvordan vægtene skal ændres for at opnå en mindre tab ved næste iteration (forbedre modellen).\n",
    "\n",
    "Gradienterne er tilføjelser til vægtene, $\\Delta w_1, \\Delta w_2, \\Delta w_3, \\Delta w_4$ og til bias, $\\Delta b$, som bruges til at beregne nye vægte for modellen. Den enkleste måde at beregne de nye vægte er følgende:\n",
    "\n",
    "$w_1 = w_1 - \\alpha \\Delta w_1$<br>\n",
    "$w_2 = w_2 - \\alpha \\Delta w_2$<br>\n",
    "$w_3 = w_3 - \\alpha \\Delta w_3$<br>\n",
    "$w_4 = w_4 - \\alpha \\Delta w_4$<br>\n",
    "$bias = bias - \\alpha \\Delta b$\n",
    "\n",
    "Som du kan se ovenfor, bruges tilføjelserne ikke direkte, men der er en yderligere parameter $\\alpha$, som skal være mellem 0 og 1. For eksempel, hvis $\\alpha = 0.01$, vil ændringen i vægtene være 1% af den beregnede tilføjelse.\n",
    "\n",
    "Parametren $\\alpha$ kaldes en *læringsrate*, og den er nødvendig for at sænke læringsprocessen og gøre den mere jævn. Du vil se nogle eksempler senere i denne klasse.\n",
    "\n",
    "Gradienterne beregnes af tabsfunktionsobjektet (i eksemplet ovenfor er det `nn.MSELoss()`), mens opdateringen af vægtene udføres af en anden funktion, en *optimerer*. Der er flere optimeringsfunktioner tilgængelige, nomalt er en af de følgende to et godt valg:\n",
    "\n",
    "* *GD* eller *SGD* (stokastisk gradientnedstigning) er den enkleste optimeringsalgoritme, som fungerer præcis som vist i ligningerne ovenfor. Enkel og ligetil. Navnet *gradientnedstigning* betyder, at du beregner gradienten for at finde trin for vægtene, som får tabet til at gå nedad (nedstigning).\n",
    "* *Adam* er en mere sofistikeret optimeringsalgoritme, der opdaterer vægtene på en lidt mere kompleks måde end vist ovenfor. Den har flere parametre at finjustere og er normalt mere effektiv.\n",
    "\n",
    "I alle eksempler vil vi bruge *SGD*, da det er mere enkelt, men du kan prøve *Adam* senere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Træning af ANN model\n",
    "\n",
    "Ideen med træning af ANN er som følger:\n",
    "\n",
    "1. Anvend ANN til at beregne output baseret på inputs fra træningssættet.\n",
    "2. Beregn tab baseret på det nuværende output og de ønskede outputværdier.\n",
    "3. Beregn gradienter og opdater vægte, således at tabet næste gang er mindre.\n",
    "\n",
    "Det første skridt kaldes *forward propagation* fordi data-værdier flyder fra venstre (inputs) til højre (outputs) gennem alle neuroner imellem (vi har kun én neuron indtil videre i vores model, men det betyder ikke noget — det, der virker for én, vil virke for tusindvis).\n",
    "\n",
    "Det andet trin er processen med at beregne forskellen mellem de forudsagte værdier og de faktiske værdier baseret på de indledende værdier for vægt og bias for hver neuron. Som tidligere nævnt er det tabet, der skal minimeres.\n",
    "\n",
    "Det tredje skridt kaldes *back propagation*, fordi gradienterne beregnes for output-neuronen først. Derefter for neuronerne tilbage mod outputtet, og så videre, indtil alle neuroner får opdateret de nye vægte baseret på gradienterne.\n",
    "\n",
    "Disse tre trin gentages, indtil et kriterium er opfyldt, f.eks. at tabet ikke bliver mindre længere. Hver gang en model udfører alle tre trin for alle rækker i træningssættet, tager det en *epoke*. Så hvis du kører træningsprocessen i 10 epoker, betyder det, at disse tre trin gentages 10 gange for alle rækker i træningssættet.\n",
    "\n",
    "Lad os implementere dette for vores model. Først og fremmest lad os forberede X- og y-værdier fra vores træningssæt (så de er numeriske og konverteret til Torch tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select X variables, convert them to torch tensor and make them to have type float\n",
    "X_train_values = d_train.iloc[:, 1:5].values\n",
    "X_train = torch.tensor(X_train_values).float()\n",
    "\n",
    "# compare species values with target class to get logical target values\n",
    "c_train = d_train[\"Species\"]\n",
    "\n",
    "# convert logical values to numeric, so it is +1 if flower is virginica and -1 otherwise\n",
    "y_train_dummy = (c_train  == \"virginica\") * 2.0 - 1.0\n",
    "\n",
    "# convert the numeric values to torch tensor, make them float and transpose to a column\n",
    "y_train = torch.tensor([y_train_dummy.values]).float().t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu skal vi definere antallet af epoker til træning og tabfunktionen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "nepochs = 20\n",
    "\n",
    "\n",
    "# define a loss function\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvor mange epoker der skal bruges, afhænger af sagen, normalt kan 100 epoker bruges som udgangspunkt. Vi vil bruge 20 bare for at gøre outputtet kortere.\n",
    "\n",
    "Vi er klar til at træne modellen.\n",
    "\n",
    "Før vi starter, vil vi fastsætte tilstanden af tilfældighedsgenerator (som bruges til at initialisere vægtene). Dette er nødvendigt for at få reproducerbare resultater nedenfor, så når vi genkører koden, vil resultaterne være de samme. Hvis du senere vil prøve denne kode med virkelig tilfældige vægte, skal du blot udkommentere disse to linjer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fix the state of random numbers generator\n",
    "seed = 11\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# initialize a new model\n",
    "model = SimpleModel()\n",
    "\n",
    "# define optimizer which will compute gradients — do the learning, back propagation.\n",
    "#\n",
    "# parameter \"lr\" is learning rate it tells how large changes\n",
    "# the weights will have (so it regulates how fast the learning process is)\n",
    "# - if \"lr\" is too small your model can stuck and never reach the optimal model\n",
    "# - if \"lr\" is too large your model can overshoot the optimal model\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# set model to training mode\n",
    "model.train()\n",
    "\n",
    "# training loop\n",
    "for epoch in range(nepochs):\n",
    "\n",
    "    # the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 1. forward pass\n",
    "    y_hat = model(X_train)\n",
    "\n",
    "    # 2. compute the loss\n",
    "    loss = loss_function(y_hat, y_train)\n",
    "\n",
    "    # 3. backward pass and optimize weights\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # show how big the loss is\n",
    "    print(f'Epoch {epoch}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis du kommenterer linjen, der fryser tilfældigt nummergenerator, og klikker på knappen \"kør\" flere gange, vil du se, at tabsværdierne er forskellige hver gang du geninitialiserer og træner din model. Det skyldes, at de indledende vægte er indstillet til tilfældige tal, så træningsresultatet ikke er forudbestemt.\n",
    "\n",
    "Derfor er det vigtigt at fryse tilstanden af hensyn til læring. Fjern venligst kommentaren og sørg for, at resultaterne er reproducerbare.\n",
    "\n",
    "Lad os se, hvordan det klarer sig på træningssættet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model to predictions mode\n",
    "model.eval()\n",
    "\n",
    "# apply model to training set\n",
    "y_hat = model.forward(X_train)\n",
    "\n",
    "# convert output to numpy array\n",
    "y_hat_arr = y_hat.t().detach().numpy()\n",
    "\n",
    "# combine with reference values\n",
    "res = pd.DataFrame({\n",
    "    \"Reference\": d_train[\"Species\"],\n",
    "    \"Predicted\": y_hat_arr[0]\n",
    "})\n",
    "\n",
    "# compute statistics\n",
    "TP = sum((res[\"Reference\"] == \"virginica\") & (res[\"Predicted\"] > 0))\n",
    "TN = sum((res[\"Reference\"] != \"virginica\") & (res[\"Predicted\"] < 0))\n",
    "FP = sum((res[\"Reference\"] != \"virginica\") & (res[\"Predicted\"] > 0))\n",
    "FN = sum((res[\"Reference\"] == \"virginica\") & (res[\"Predicted\"] < 0))\n",
    "\n",
    "sens = TP / (TP + FN)\n",
    "spec = TN / (TN + FP)\n",
    "acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "(sens, spec, acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ikke dårligt for selvtræning på 20 epoker. Og da dette er en meget simpel en-neuronsmodel, kan du få og se på vægtene og bias for denne neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show weights and bias of the trained model\n",
    "(model.layer1.weight, model.layer1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som du kan se, er den vigtigste input i dette tilfælde *Petallængde*, da den har den største vægt på $0.56$. Den mindst vigtige er den anden (*Sepalbredde*), som har en vægt på $-0.03$. Den første input (*Sepallængde*) bidrager også negativt i denne kombination, ligesom den sidste.\n",
    "\n",
    "Med andre ord implementerer vores kunstige neurale netværk følgende lineære model:\n",
    "\n",
    "$\\hat{Y} = -0.36 \\times X_1 - 0.03 \\times X_2 + 0.56 \\times X_3 - 0.27 \\times X_4 + 0.07$\n",
    "\n",
    "Nu kan vi anvende vores model på testsættet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare X and y values for the training set\n",
    "X_test = torch.tensor(d_test.iloc[:, 1:5].values).float()\n",
    "\n",
    "c_test = d_test[\"Species\"]\n",
    "y_test = (c_test == \"virginica\") * 2.0 - 1.0\n",
    "y_test = torch.tensor([y_test.values]).float().t()\n",
    "\n",
    "# apply model to test set\n",
    "y_hat = model.forward(X_test)\n",
    "\n",
    "# convert output to numpy array\n",
    "y_hat_arr = y_hat.t().detach().numpy()\n",
    "\n",
    "# combine with reference values\n",
    "res = pd.DataFrame({\n",
    "    \"Reference\": c_test,\n",
    "    \"Predicted\": y_hat_arr[0]\n",
    "})\n",
    "\n",
    "# compute statistics\n",
    "TP = sum((res[\"Reference\"] == \"virginica\") & (res[\"Predicted\"] > 0))\n",
    "TN = sum((res[\"Reference\"] != \"virginica\") & (res[\"Predicted\"] < 0))\n",
    "FP = sum((res[\"Reference\"] != \"virginica\") & (res[\"Predicted\"] > 0))\n",
    "FN = sum((res[\"Reference\"] == \"virginica\") & (res[\"Predicted\"] < 0))\n",
    "\n",
    "sens = TP / (TP + FN)\n",
    "spec = TN / (TN + FP)\n",
    "acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "(sens, spec, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testsættet fungerer det endnu bedre (måske fordi testsættet er mindre).\n",
    "\n",
    "### Øvelse 2\n",
    "\n",
    "Nu leg lidt med denne kode. Forsøg at fjerne `torch.manual_seed()` instruktionen og kør træningen flere gange. Prøv at øge antallet af epoker og se, hvordan det påvirker kvaliteten af klassificeringen både for træningssættet og testsættet. Prøv at ændre læringshastigheden (gør den mindre eller større). Få den bedst mulige model og rapporter resultaterne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aktiveringsfunktion og neuronlag\n",
    "\n",
    "Hvordan gør man modellen endnu mere effektiv? Det mest oplagte er at øge antallet af lag og forbinde dem alle sammen. Dog vil det samlede antal vægte i dette tilfælde multipliceres, og du kan ende op med en model med tusindvis af vægte at finjustere. I dette tilfælde har du brug for flere objekter for at kunne træne, validere og teste det ordentligt.\n",
    "\n",
    "Den anden måde at gøre en ANN-model mere effektiv er at gøre den ikke-lineær. Den enkleste måde at introducere ikke-linearitet på er at supplere hver lineær neuron med hvad der kaldes en *aktiveringsfunktion*.\n",
    "\n",
    "Aktiveringsfunktionen ændrer outputtet afhængigt af dets værdi. Den simple aktiveringsfunktion kaldes **Rectified Linear Unit (ReLU)**. Den fungerer som følger: Hvis outputtet er negativt, gør den det lig med nul. Men når outputtet er positivt, beholder den det bare som det simpelthen, som det er.\n",
    "\n",
    "På billedet nedenfor kan du se to forskellige aktiveringsfunktioner. Den blå er ReLU:\n",
    "\n",
    "<img src=\"./illustrations/ReLU_and_GELU.svg\" style=\"width:300px\">\n",
    "\n",
    "Lad os implementere ANN med 3 lag. Det første lag vil bestå af 8 neuroner. Hver neuron vil have 4 inputs og 1 output, så dette lag vil have 8 outputs. Det andet lag vil bestå af 4 neuroner, hver neuron har 8 inputs og 1 output, så dette lag vil have fire outputs. Endelig vil det sidste lag være det samme som vi har i vores nuværende model — én neuron med 4 inputs og 1 output.\n",
    "\n",
    ">**Note til læreren**<br>tegn arkitekturen på en tavle.\n",
    "\n",
    "Neuroner i de to første lag vil også have ReLU aktiveringsfunktion for outputtet.\n",
    "\n",
    "Her er implementeringen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class NewModel(nn.Module):\n",
    "    \"\"\" class for three layers ANN \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NewModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(4, 8)  # first layer: 4 inputs and 8 outputs\n",
    "        self.layer2 = nn.Linear(8, 4)  # second layer: 8 inputs and 4 outputs\n",
    "        self.layer3 = nn.Linear(4, 1)  # third layer: 4 inputs and 1 output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x)) # pass inputs through the first layer and apply ReLU activation\n",
    "        x = F.relu(self.layer2(x)) # pass output from layer 1 through the second layer + ReLU activation\n",
    "        y_hat = self.layer3(x) # pass output from layer 2 through the third layer (no relu)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Og her er koden, der implementerer resten (træning og test).\n",
    "\n",
    "Som du kan se i dette tilfælde, beregner vi for hvert epoch tabet for både træningssættet og testsættet. Så vi kan opdage situationer, hvor modellen bliver dårligere for testsættet og stoppe. Denne proces kaldes *validering* og det lader os undgå overtræning af modellen, når den fungerer perfekt for træningssættet, men dårligt for testsættet. Denne situation kaldes også *overfitting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use another manual seed here to get reproducible outcome\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# number of epochs to train the model\n",
    "nepochs = 300\n",
    "\n",
    "# define a loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# initialize the new model\n",
    "model = NewModel()\n",
    "\n",
    "# define optimizer which will compute gradients — do the learning.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(nepochs):  # Number of training epochs\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_hat_train = model(X_train)\n",
    "    train_loss = loss_function(y_hat_train, y_train)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # validate\n",
    "    model.eval()\n",
    "    y_hat_test = model(X_test)\n",
    "    test_loss = loss_function(y_hat_test, y_test)\n",
    "\n",
    "    # show how big the loss is\n",
    "    print(f'Epoch {epoch}, train loss: {train_loss.item():.4f} - test loss {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som du kan se, bruger vi flere epoker i dette tilfælde, da modellen er lidt mere kompleks.\n",
    "\n",
    "Lad os se, hvordan den klarer sig på testsættet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model to prediction mode\n",
    "model.eval()\n",
    "\n",
    "# apply model to test set\n",
    "y_hat = model.forward(X_test)\n",
    "\n",
    "# convert output to numpy array\n",
    "y_hat_arr = y_hat.t().detach().numpy()\n",
    "\n",
    "# combine with reference values\n",
    "res = pd.DataFrame({\n",
    "    \"Reference\": c_test,\n",
    "    \"Predicted\": y_hat_arr[0]\n",
    "})\n",
    "\n",
    "# compute statistics\n",
    "TP = sum((res[\"Reference\"] == \"virginica\") & (res[\"Predicted\"] > 0))\n",
    "TN = sum((res[\"Reference\"] != \"virginica\") & (res[\"Predicted\"] < 0))\n",
    "FP = sum((res[\"Reference\"] != \"virginica\") & (res[\"Predicted\"] > 0))\n",
    "FN = sum((res[\"Reference\"] == \"virginica\") & (res[\"Predicted\"] < 0))\n",
    "\n",
    "sens = TP / (TP + FN)\n",
    "spec = TN / (TN + FP)\n",
    "acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "(sens, spec, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som vi nævnte ovenfor, strengt taget skal vi til dette sidste trin bruge et andet sæt prøver, uafhængigt af det der blev brugt til træning og validering. Vi bryder denne regel her udelukkende for illustrative formål, kun fordi vores datasæt er lille.\n",
    "\n",
    "Vi kan visualisere de forudsagte værdier for bedre forståelse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(res[\"Reference\"], res[\"Predicted\"])\n",
    "plt.plot(plt.xlim(), [0, 0], color = \"black\", linestyle=\"--\")\n",
    "plt.ylabel(\"Predicted output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det skal bemærkes, at hverken netværket vi bruger i det sidste eksempel, eller den valgte tabssfunktion er specifikt god til klassificering. Men selv med denne valg ser resultatet godt ud. Lad os lære, hvordan vi kan forbedre dette."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiklasse klassifikation\n",
    "\n",
    "En af de mest almindelige måder at lave en klassifikationsmodel med en kunstig neuralt netværk (ANN) på, er ved at bruge outputlaget med flere neuroner - ét for hver klasse. Så, for binær klassifikation får vi to outputs, for klassifikation blandt tre klasser - tre og så videre. Men hvordan træffes klassifikationsbeslutningen i dette tilfælde, og hvilken tabfunktion skal bruges?\n",
    "\n",
    "Idéen er lignende afstemning. Beslutningen træffes ved at vælge outputtet med den største værdi. For eksempel, hvis de forudsagte værdier i outputlaget er [0.23, 0.89, -0.01], så \"vinder\" det andet output, og den forudsagte labels indeks vil være 1 (husk, i Python starter indekser fra 0, så vi har 0, 1, og 2 i stedet for 1, 2, og 3). Dette betyder følgende:\n",
    "\n",
    "1. Vi skal have lige så mange outputs i det sidste (output)lag, som vi har klasser.\n",
    "2. Vi skal oprette en vektor med referenceklassernes indeks.\n",
    "3. Vi skal bruge en speciel tabfunktion, der fungerer bedst i dette tilfælde.\n",
    "\n",
    "Lad os implementere dette ved at oprette en model, der vil forudsige klasselabelen for Iris-dataene. Denne gang nogen af de tre labels. Lad os antage, at labelen `\"setosa\"` vil have indeks 0, `\"versicolor\"` vil have indeks 1, og `\"virginica\"` vil have indeks 2.\n",
    "\n",
    "Lad os oprette en ordbog med indeksene og labelerne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector with classes, so we can get a label by its index\n",
    "classes = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "\n",
    "# create dictionary so we can get index by label\n",
    "class_to_idx = {\"setosa\": 0, \"versicolor\": 1, \"virginica\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu skal vi forberede datasættene. Vi har allerede gjort dette ovenfor, men lad os gøre det igen, og denne gang lad os også oprette en vektor med referenceklasseindeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensor with predictor values\n",
    "X_values = d.iloc[:, 1:5].values\n",
    "X = torch.tensor(X_values).float()\n",
    "\n",
    "# create tensor with class indices\n",
    "label_values = [class_to_idx[label] for label in d[\"Species\"]]\n",
    "labels = torch.tensor(label_values).long()\n",
    "\n",
    "# show it on the screen to check\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som du husker, gør tilføjelse af `float()` til tensoren værdierne til flydende kommatalstal. Tilføjelse af `long()` gør dem til heltalsværdier. At have label indekser som heltal er påkrævet af tabfunktionen, vi kommer til at bruge.\n",
    "\n",
    "Nu vil vi splitte begge tensors op i trænings- og testsæt ligesom vi gjorde tidligere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate indices of rows for training and test set\n",
    "train_ind = d[\"Id\"] % 5 != 0\n",
    "test_ind = d[\"Id\"] % 5 == 0\n",
    "\n",
    "# select rows and label indices for the training set\n",
    "X_train = X[train_ind, :]\n",
    "labels_train = labels[train_ind]\n",
    "\n",
    "# select rows and label indices for the test set\n",
    "X_test = X[test_ind, :]\n",
    "labels_test = labels[test_ind]\n",
    "\n",
    "# show test set labels\n",
    "labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som du kan se, denne gang i stedet for at lave delmængder for data rammen, vi oprettede tensorer først og derefter delmængde tensoerne. Denne måde er lidt kortere og måske mere klar.\n",
    "\n",
    "Nu vil vi oprette en ny klasse med ANN model med tre outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiClassModel(nn.Module):\n",
    "    \"\"\" class for three layers ANN \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MultiClassModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(4, 8)  # first layer: 4 inputs and 8 outputs\n",
    "        self.layer2 = nn.Linear(8, 16)  # first layer: 8 inputs and 16 outputs\n",
    "        self.layer3 = nn.Linear(16, 3)  # third layer: 16 inputs and 3 outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x)) # pass inputs through the first layer and apply ReLU activation\n",
    "        x = F.relu(self.layer2(x)) # pass output from layer 1 through the second layer + ReLU activation\n",
    "        y_hat = self.layer3(x) # pass output from layer 2 through the third layer (no relu)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som du kan se, modellen ligner meget det, vi havde før, vi har blot ændret antallet af input/output.\n",
    "\n",
    "> **Bemærkning til læreren:**<br>tegn modellen skematisk på en tavle.\n",
    "\n",
    "Nu skal vi definere tabsfunktionen. I dette særlige tilfælde, når beslutningen træffes ved afstemning, er den bedst egnede funktion cross-entropy-tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a loss function\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alt er klar, lad os initialisere og træne modellen (ligesom i eksemplerne ovenfor vil vi fryse tilfældighedsgeneratorens tilstand for at opnå reproducerbare resultater):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed the random number generator to get reproducible outcome\n",
    "torch.manual_seed(12)\n",
    "\n",
    "# initialize the new model\n",
    "model = MultiClassModel()\n",
    "\n",
    "# define optimizer which will compute gradients — do the learning.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# number of epochs to train the model\n",
    "nepochs = 100\n",
    "\n",
    "for epoch in range(nepochs):  # Number of training epochs\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    labels_predicted = model(X_train)\n",
    "\n",
    "    loss = loss_function(labels_predicted, labels_train)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    # validate\n",
    "    model.eval()\n",
    "    labels_predicted = model(X_test)\n",
    "    loss = loss_function(labels_predicted, labels_test)\n",
    "    val_loss = loss.item()\n",
    "\n",
    "    # show how big the loss is\n",
    "    print(f'Epoch {epoch}, train loss: {train_loss:.4f} - validation loss {val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indtil videre går det godt, selvom som du kan se, fortsætter den validerende taben med at falde, så måske er 100 epoker ikke nok. Vi vil vende tilbage til dette senere. Lad os lære, hvordan man laver forudsigelser.\n",
    "\n",
    "Lad os lave forudsigelser for begge sæt og se på de forudsagte værdier for testsættet først:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model to prediction mode\n",
    "model.eval()\n",
    "\n",
    "# apply model to train and test sets\n",
    "output_train = model.forward(X_train)\n",
    "output_test = model.forward(X_test)\n",
    "\n",
    "# show predictions for the test set\n",
    "output_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis du kigger nøje efter, kan du finde ud af, at for de første ti rækker (hvor vi har *setosa* prøver) er den største af de tre værdier faktisk den første. For de to andre er det ikke så tydeligt. Lad os anvende `max` funktionen på hver række og spørge ikke efter værdien, men efter positionen (indeks) hvor den største værdi befinder sig:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of largest values for each row of computed outputs\n",
    "_, predictions_train = torch.max(output_train, 1)\n",
    "_, predictions_test = torch.max(output_test, 1)\n",
    "\n",
    "predictions_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ja, den forudsagte klasselabel-indeks for setosa er god, men de andre er ikke helt perfekte endnu. Vi vil fikse det senere.\n",
    "\n",
    "Fordi vi her har tre klasser, vil det være for tidskrævende at beregne klassifikationsstatistikker for hver enkelt. Lad os lære en ny måde at vurdere klassificeringsresultaterne på — via en contingency-tabel eller [forvirringsmatrix](https://da.wikipedia.org/wiki/Forvirringsmatrix). Den viser simpelthen alle mulige kombinationer af reference- og forudsagte klasselabels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# compute contingency table for training and test sets\n",
    "ct_train = np.zeros((3, 3))\n",
    "ct_test = np.zeros((3, 3))\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ct_train[i, j] = sum((labels_train == i) & (predictions_train == j))\n",
    "        ct_test[i, j] = sum((labels_test == i) & (predictions_test == j))\n",
    "\n",
    "(ct_train, ct_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabellen for den ideelle klassifikation skal have nul for alle værdier undtagen på diagonalen (hvor det forudsagte og reference-labelindekset matcher). Som du kan se, har vi en perfekt match for den første (0, \"setosa\") og den anden (2, \"versicolor\") klasse, men \"virginica\" klassen er ikke forudsagt godt. Seks blomster af denne klasse blev forkert forudsagt som *versicolor* og fire blev korrekt forudsagt som *virginica*.\n",
    "\n",
    "Du kan også beregne denne tabel for relative værdier (procent), hvilket er nemmere at bruge, når antallet af individer i forskellige klasser ikke er det samme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute contingency table for training and test set with relative values\n",
    "ct_train = np.zeros((3, 3))\n",
    "ct_test = np.zeros((3, 3))\n",
    "for i in range(3):\n",
    "    n_train = sum(labels_train == i)\n",
    "    n_test = sum(labels_test == i)\n",
    "    for j in range(3):\n",
    "        ct_train[i, j] = sum((labels_train == i) & (predictions_train == j)) / n_train\n",
    "        ct_test[i, j] = sum((labels_test == i) & (predictions_test == j)) / n_test\n",
    "\n",
    "(ct_train, ct_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lad os lære at visualisere dette ved at lave en varmekort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap plot for contrast table\n",
    "plt.imshow(ct_test, clim = [0, 1])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu skal vi tilføje klasselabels og vise tallene for hver celle i tabellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap plot for contrast table\n",
    "plt.imshow(ct_test, clim = [0, 1])\n",
    "plt.colorbar()\n",
    "plt.gca().set_xticks(range(3), classes)\n",
    "plt.gca().set_yticks(range(3), classes)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        plt.text(i, j, round(ct_test[j, i], 3), color = \"white\" if ct_test[j, i] < 0.5 else \"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lad os oprette flere funktioner for at kunne genbruge koden, vi har skrevet ovenfor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, labels_train, X_val, labels_val, nepochs = 100, lr = 0.01):\n",
    "    \"\"\" trains any classification model using provided data, number of epochs and learning rate \"\"\"\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        labels_predicted = model(X_train)\n",
    "        loss = loss_function(labels_predicted, labels_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        labels_predicted = model(X_val)\n",
    "        loss = loss_function(labels_predicted, labels_val)\n",
    "        val_loss = loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch}, train loss: {train_loss:.4f} - validation loss {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X):\n",
    "    \"\"\" get ANN model and tensor with predictors and returns predicted class label indices \"\"\"\n",
    "    model.eval()\n",
    "    output = model.forward(X)\n",
    "    _, labels = torch.max(output, 1)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table(reference, predicted):\n",
    "    \"\"\" computes contingency table for predicted and reference class label indices \"\"\"\n",
    "    indices = reference.unique()\n",
    "    n = len(indices)\n",
    "    ct = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        ni = sum(reference == indices[i])\n",
    "        for j in range(n):\n",
    "            ct[i, j] = sum((reference == indices[i]) & (predicted == indices[j])) / ni\n",
    "\n",
    "    return ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ct_heatmap(ct, classes):\n",
    "    \"\"\" shows heatmap for the contingency table \"\"\"\n",
    "    plt.imshow(ct, clim = [0, 1])\n",
    "    plt.colorbar()\n",
    "\n",
    "    n = len(classes)\n",
    "    plt.gca().set_xticks(range(n), classes)\n",
    "    plt.gca().set_yticks(range(n), classes)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            plt.text(i, j, round(ct[j, i], 3), color = \"white\" if ct[j, i] < 0.5 else \"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som du kan se, returnerer funktionen `train()` ingenting. Det er fordi `model`, som vi passerer til denne funktion som det første argument, er et objekt, der har sine egne metoder, som opdaterer dette objekt internt. Så selvom al træning sker inde i denne funktion, får objektet udenfor alle nødvendige opdateringer.\n",
    "\n",
    "Desuden kan vi også bruge denne funktion til enhver anden model.\n",
    "\n",
    "Lad os se, hvordan de nye funktioner virker. Lad os træne og initialisere modellen igen og træne den igen med flere epoker og en mindre læringsrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12)\n",
    "model = MultiClassModel()\n",
    "train(model, X_train, labels_train, X_test, labels_test, nepochs = 1000, lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Og tjek hvordan det klarer sig:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = predict(model, X_train)\n",
    "predictions_test = predict(model, X_test)\n",
    "\n",
    "ct_train = table(labels_train, predictions_train)\n",
    "ct_test = table(labels_test, predictions_test)\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "ct_heatmap(ct_train, classes)\n",
    "plt.title(\"Train\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "ct_heatmap(ct_test, classes)\n",
    "plt.title(\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med 1000 epoker og en mindre læringsrate fungerer den nye arkitektur næsten perfekt!\n",
    "\n",
    "### Øvelse 3\n",
    "\n",
    "I filen `IrisHeatmap.csv` finder du nye datapunkter, som indeholder nye målinger, men ikke har en kolonne med art eller en kolonne med ID'er (så den har kun fire kolonner med målinger). Indlæs dataene fra filen, og anvend derefter det ANN, du lige har trænet, for at få forudsigelserne.\n",
    "\n",
    "Efterfølgende lav et spredningsplot, hvor x- og y-værdierne er Petalslængde og Petalsbredde, som du har fået fra filen. Vis punkter forudsagt som *setosa*, ved hjælp af rød farve, punkter forudsagt som *versicolor* som grønne, og punkter forudsagt som *virginica* som blå. Kommenter plottet. Hvad viser det faktisk?\n",
    "\n",
    "Prøv at tilføje forudsigelser for trænings- og testsættet til plottet. I dette tilfælde bliver du nødt til at gøre forudsigelserne for de nye data vist semitransparente. Brug parameteren `alpha = 0.15` i `plt.scatter()`-funktionen til dette."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisering af træningsprocessen\n",
    "\n",
    "Det kan være nyttigt at se, hvordan trænings- og valideringstab ændrer sig med epokerne. Lad os modificere metoden `train_model()` for at indsamle denne information og returnere den til brugeren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, labels_train, X_test, labels_test, nepochs = 100, lr = 0.01):\n",
    "    \"\"\" trains any classification model using provided data, number of epochs and learning rate \"\"\"\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    train_losses = np.zeros(nepochs)\n",
    "    val_losses = np.zeros(nepochs)\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        labels_predicted = model(X_train)\n",
    "        loss = loss_function(labels_predicted, labels_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses[epoch] = loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        labels_predicted = model(X_test)\n",
    "        loss = loss_function(labels_predicted, labels_test)\n",
    "        val_losses[epoch] = loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch}, train loss: {train_losses[epoch]:.4f} - validation loss {val_losses[epoch]:.4f}')\n",
    "\n",
    "    return (train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu skal vi træne modellen igen, denne gang ved hjælp af 5000 epoker, få tabsværdierne og lave en plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed the random number generator to get reproducible outcome\n",
    "torch.manual_seed(12)\n",
    "\n",
    "# re-initialize and train the model\n",
    "model = MultiClassModel()\n",
    "train_losses, val_losses = train(model, X_train, labels_train, X_test, labels_test,\n",
    "                                 nepochs = 5000, lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show plot with both losses\n",
    "plt.plot(train_losses, label = \"train\")\n",
    "plt.plot(val_losses, label = \"val\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim([0, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som du kan se, ændrer valideringstab næsten ikke efter cirka 3000 epoker. Så der er ingen grund til at køre modellen så langt.\n",
    "\n",
    "Lad os geninitialisere modellen (dette er nødvendigt for at sætte alle initiale vægte til tilfældige tal; ellers vil vægte til allerede trænet model blive brugt som udgangspunkt) og træne den igen, men denne gang vil vi bruge en stor læringsrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12)\n",
    "model = MultiClassModel()\n",
    "train_losses, val_losses = train(model, X_train, labels_train, X_test, labels_test,\n",
    "                                 nepochs = 5000, lr = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label = \"train\")\n",
    "plt.plot(val_losses, label = \"val\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kan se nogle mærkelige mønstre, \"toppe\", på plottet. Disse mønstre er typiske for en stor læringsrate, hvor optimeringsalgoritmen ændrer vægtene på neuronerne for meget, så modellen \"springer\" frem og tilbage.\n",
    "\n",
    "Dette kan sammenlignes med at køre en bil. Hvis du accelererer og bremser langsomt, overvåger situationen på vejen og handler i god tid (proaktivt), vil din bil bevæge sig jævnt og behageligt for dig og dine passagerer. Men situationen vi ser på plottet ovenfor svarer til tilfældet hvor du accelererer og bremser konstant.\n",
    "\n",
    "Dette plot hjælper med at finde den optimale læringsrate og antal epoker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gem og indlæs modeltilstand\n",
    "\n",
    "Endelig, lad os lære, hvordan man gemmer og indlæser modeltilstanden - alle interne parametre, som inkluderer vægte og bias af alle lag, osv. Dette kan være nyttigt i følgende situationer:\n",
    "\n",
    "1. Du kan gemme modeltilstanden i en separat variabel efter hvert epoch, men kun hvis valideringstab bliver bedre. Dette hjælper med at undgå overfitting når du træner din model for længe, og den klarer sig dårligere på valideringssættet end f.eks. 50 epoch'er før. Vi vil overveje dette tilfælde i den næste klasse.\n",
    "\n",
    "2. Gem modellen til en fil til senere brug. For eksempel kan du sende den til dine klassekammerater eller kolleger. Eller bare gem den for at fortsætte træningsprocessen i morgen. Eller for at genbruge den til forudsigelser. Mange muligheder.\n",
    "\n",
    "Lad os først se, hvordan man får modellens tilstand. Prøv at køre den næste kode først:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in model.parameters():\n",
    "    print(parameter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kan se en samling af torch tensors (ligesom NumPy-arrays, som vi husker). For eksempel har den første tensor 8 rækker og 4 kolonner. Kan du gætte hvorfor?\n",
    "\n",
    "Fordi i den første lag af din model har du 8 neuroner. Hvert neuron har 4 inputs og 1 output. Og output beregnes ved at tage en vægtet sum af inputs plus bias. Så for hvert neuron har du brug for 5 parametre — 4 vægte og 1 bias.\n",
    "\n",
    "Den første tensor af størrelse 8x4 indeholder vægten for hver af de 8 neuroner, og den anden tensor (en vektor med 8 værdier) indeholder biases.\n",
    "\n",
    "Det andet lag har 16 neuroner, hvert neuron har 8 inputs og 1 output. Så hvert neuron har 9 parametre — 8 vægte og 1 bias. Det er præcis hvad du har i det næste par af tensors, den ene af størrelse 16x8 indeholder vægte og den ene med 16 værdier indeholder biases.\n",
    "\n",
    "Endelig har den sidste lag 3 neuroner med 16 inputs og 1 output hver, så de sidste to tensors indeholder vægte og biases for disse neuroner.\n",
    "\n",
    "Faktisk kan du se strukturen af din model og antallet af parametre ved at bruge metoden `summary()` fra biblioteket `torchinfo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kan også beregne det samlede antal parametre i din model manuelt ved simpelthen at tage summen af alle elementer i hver tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npar = 0\n",
    "for parameter in model.parameters():\n",
    "    npar = npar + parameter.numel()\n",
    "\n",
    "print(f\"Total number of parameters in this ANN: {npar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kan selvfølgelig se alle vægtene (outputtet vil dog være langt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in model.parameters():\n",
    "    print(\"------\")\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strengt taget kan du tage alle disse værdier, kopiere dem til Excel og køre forudsigelser i Excel (du skal blot huske at anvende ReLU-funktionen på hver output). Skør idé, men det kan lade sig gøre. Dette viser, at ANN ikke er raketvidenskab, men en meget simpel, ligetil og alligevel kraftfuld metode.\n",
    "\n",
    "Der er en anden måde at få modelparametrene med al nødvendig yderligere information på — statens ordbog. Der findes en metode til det:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som du kan se, er outputtet lignende med det, vi har set før, men denne gang er det organiseret som en ordbog, så Torch vil kende et niveaunavn og parameternavn.\n",
    "\n",
    "Vi kan gemme tilstanden i en variabel ved at tage en dyb kopi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to load a special function which creates copy of complex objects\n",
    "from copy import deepcopy\n",
    "\n",
    "# save parameters of current model to a variable\n",
    "model_state = deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvorfor har du brug for at tage en dyb kopi i stedet for bare at tildele tilstanden til en ny variabel? Fordi hvis du fortsætter med at træne din model, vil denne tilstand også få alle opdateringer. Ved at tage en dyb kopi adskiller du på en måde den nuværende tilstand fra de næste, hvilket gør den uafhængig.\n",
    "\n",
    "Her er et eksempel på, hvordan du kan bruge det:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed the random number generator to get reproducible outcome\n",
    "torch.manual_seed(12)\n",
    "\n",
    "# initialize a new model\n",
    "new_model = MultiClassModel()\n",
    "\n",
    "# make predictions — they will be very bad because the model is not trained\n",
    "predictions = predict(new_model, X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets load the parameters we saved from the trained model to this new model\n",
    "new_model.load_state_dict(model_state)\n",
    "\n",
    "# and make predictions\n",
    "predictions = predict(new_model, X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Og nu har vi perfekte forudsigelser uden at træne den nye model, men bare ved at genbruge parametrene fra den tidligere trænede model.\n",
    "\n",
    "Hvis du vil gemme tilstanden til en fil og sende den til nogen, eller indlæse den senere i en anden Python-script, kan du bruge to PyTorch-funktioner: `torch.save()` gemmer modeldictionary til en fil, og `torch.load()` indlæser den fra filen. Filen skal have filtypenavnet `.pth`.\n",
    "\n",
    "Lad os se, hvordan det virker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save state dictionary to file\n",
    "torch.save(model.state_dict(), \"mymodel.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som du kan se, behøver vi ikke at tage en deepcopy i dette tilfælde, fordi det vil være i en separat fil. Hvis du kører det, får du filen 'mymodel.pth' i den nuværende mappe.\n",
    "\n",
    "Der er ingen grund til at åbne den, da informationen indeni er kodet ved hjælp af binær format. Men du kan indlæse den og tildele den til modellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new model with random weights\n",
    "another_model = MultiClassModel()\n",
    "\n",
    "# load state from the file and assign it to the new model\n",
    "model_state = torch.load(\"mymodel.pth\")\n",
    "another_model.load_state_dict(model_state)\n",
    "\n",
    "# make predictions\n",
    "predictions = predict(another_model, X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det virker!\n",
    "\n",
    "### Øvelse\n",
    "\n",
    "For at udføre denne øvelse skal du arbejde i par. En af medlemmerne skal træne ANN-netværket på Iris data (lav en separat notebook og kopier al nødvendig kode derover). Træn det ved at bruge tilfældig initialisering og prøv forskellige hyperparametre (antal epoker, indlæringshastighed osv.). Når du har fået en god model, gem den i en fil og send den til din gruppekammerat via e-mail. Din gruppekammerats opgave er at indlæse modellen og anvende den på testsettet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leg med interaktiv ANN på web\n",
    "\n",
    "Nu ved du alt, hvad du har brug for at vide til næste klasse. Vi anbefaler dog, at du leger lidt mere ved at bruge denne interaktive ANN-konstruktør, som du kan køre direkte i din webbrowser. Brug lidt tid på at eksperimentere med forskellige problemer, forskellige arkitekturer og hyperparametre:\n",
    "\n",
    "[TensorFlow playground](http://playground.tensorflow.org)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
